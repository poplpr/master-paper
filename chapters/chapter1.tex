%%==================================================
%% chapter01.tex for BIT Master Thesis
%% modified by yang yating
%% version: 0.1
%% last update: Dec 25th, 2016
%%==================================================
% 1. 绪论
%    1. 研究背景及意义
%       1. 研究背景
%       2. 研究意义
%    2. 国内外研究现状
%    3. 研究内容及主要贡献
%       1. 数据集
%       2. 分类实验
%       3. 评分实验
%    4. 论文组织结构

\chapter{绪论}
\label{chap:intro}

\section{研究背景及意义}

近年来，基于大型语言模型（Large Language Models，LLMs）的生成式自然语言处理（Generative Natural Language Processing，NLP）系统在技术上取得了显著的进展。这些进步从初期的普通语言模型，如 Transformer \cite{transformer}、GPT \cite{gpt}、GPT-2 \cite{gpt2}、GPT-3 \cite{gpt3}，逐步演化到更为复杂和强大的模型，如 ChatGPT \cite{chatgpt}、GPT-4 \cite{openai2024gpt4} 和 DeepSeek V3 \cite{deepseekai2024deepseekv3technicalreport}。这些先进的语言模型生成的文本内容与人类撰写的文本几乎无法区分，表明了人工智能在自然语言处理领域的巨大潜力。这一技术进步已经在多个领域得到了广泛应用，包括聊天机器人、自动内容生成以及文摘工具等，人工智能生成文本（AI-generated texts, AIGT）几乎渗透到我们生活的每一个角落。

然而，对于教育工作者而言，这些技术进展引发了对学生撰写文档真实性的深切担忧。在教育环境中，确保学生的作业和报告反映其真实能力和思维过程变得愈加重要。因此，如何有效避免人工智能生成文本在教育领域的滥用，成为亟待解决的关键问题 \cite{guo_how_2023}。当前的人工智能生成文本检测策略，如基于监督学习的判别器和困惑度测量方法，主要集中于判断整篇文档是否由人工智能生成。然而，现实情况是，用户往往在使用大型语言模型时对部分文本进行修改，而非完全依赖于其生成整个文档 \cite{wang_llm-detector_2024}。这种现象表明，现有检测方法在应对实际应用中的复杂性时可能显得力不从心。

因此，探索细粒度（例如句子级别）的人工智能生成文本检测显得尤为重要 \cite{wang_seqxgpt_2023}。细粒度检测不仅能够为总体检测器提供基础架构，还能够更准确地揭示文本的生成特征和模式。此外，人工智能生成文本的检测效果也可以用作评估其质量的指标：生成的文本越难以被检测出来，表明其与人类文本的一致性越高。

除了检测人工智能生成文本的能力，人工智能文本溯源的任务也逐渐浮出水面。首先，如果仅能检测出文本是由人工智能生成但无法追溯其具体来源，则该检测的说服力将大幅下降。其次，类似于人类学中的溯源概念，识别AI生成文本的来源对于理解文本的生成过程、评估其可信度，以及追踪潜在的偏见和错误至关重要。在教育学领域，为教师提供分析文本来源的工具，不仅能够帮助他们更有效地评估学生的作业和报告，还能识别文本的具体来源，从而判断学生是否过度依赖AI工具进行写作 \cite{li_origin_2023}。这一过程不仅有助于维护学术标准，还能促进学生对原创性和独立思考的重视。

通过深入了解文本的生成背景，教师能够在课堂上进行更有针对性的指导，鼓励学生发展批判性思维能力和创造性写作技巧。因此，本研究旨在探讨基于学生作文数据的人工智能生成文本分类器，旨在为教育工作者提供有效的工具，帮助他们更好地理解和评估学生的写作能力及其背后的生成机制。

\section{国内外研究现状及发展趋势}
\label{sec:intro-inout}

关于使用检测器探测文本是否由人工智能生成的任务，可以从两个主要方面进行深入探讨：文本生成与文本分类。

首先，文本生成涉及到人工智能系统如何利用特定算法和模型生成自然语言文本。这一过程通常依赖于大型语言模型（Large Language Models, LLMs）架构。通过对海量文本数据的学习，这些模型能够捕捉语言的结构和语义特征，从而生成与人类书写风格相似的文本。这一领域的研究不仅关注生成文本的质量和流畅性，还包括生成文本的多样性和上下文适应能力。因此，深入理解文本生成的机制对于评估其潜在的应用和影响至关重要。

另一方面，文本分类是指通过特定的算法和方法对文本进行分析，以确定其是否由人工智能生成。探测文本是否为人工智能生成的任务可以视为一个二分类问题，采用文本分类模型将文本标记为“人类”或“AI”。这一过程通常包括特征提取、模型训练和分类决策等多个步骤。文本分类器可以利用多种技术，包括监督学习和无监督学习方法，这些方法通过分析文本的语言特征、结构特征及其上下文信息来识别文本的生成来源。通过对文本进行分类，研究者能够揭示人工智能生成文本的特征，从而提高检测的准确性和可靠性。

\subsection{文本生成}
\label{sec:intro-inout-textgenerate}

国外在文本生成方面的技术暂时领先于国内。截至目前，大多数富有创新性的工作均由国外提出。追溯到 2017 年，Vaswani等 \cite{transformer} 提出了 Transformer 模型，其核心创新在于自注意力机制，使得模型能够有效处理长距离依赖关系。Transformer 的并行处理能力和高效的特征提取使其成为自然语言处理任务的主流选择。在自然语言处理领域，Transformer 模型的引入标志着一种新的架构设计理念的诞生，其自注意力机制使得模型能够有效捕捉序列中各个元素之间的关系。然而，传统的 Transformer 模型通常依赖于大量标注数据进行任务特定的训练，导致其在不同任务上的适应性和泛化能力受到限制。

为了解决这一问题，国外的研究人员率先开发出预训练模型。通过在大规模未标注文本上进行预训练，预训练模型不仅学习了语言的基本结构和语义，还能够通过微调快速适应多种下游任务。这种两阶段的训练策略和双向上下文理解能力，使得预训练模型在性能和效率上相较于传统模型具有显著优势，推动了自然语言处理技术的进一步发展。预训练模型在自然语言处理领域引起了广泛关注，尤其是BERT\cite{devlin_bert_2019}、BART\cite{lewis-etal-2020-bart}、GPT-1\cite{gpt}、GPT-2\cite{gpt2}和GPT-3\cite{gpt3}等模型的出现，进一步推动了技术的进步。BERT（Bidirectional Encoder Representations from Transformers）通过双向编码器架构，能够同时考虑上下文的前后信息，极大地提升了文本理解能力。BART（Bidirectional and Auto-Regressive Transformers）则结合了BERT的编码特性和GPT的自回归生成能力，适用于文本生成和序列到序列的任务。GPT-1（Generative Pre-trained Transformer 1）是首个采用自回归生成的预训练模型，奠定了生成模型的基础。随后，GPT-2在规模和性能上进行了显著提升，展示了强大的文本生成能力和广泛的应用潜力。最新的GPT-3则通过1750亿个参数，进一步提升了生成质量和上下文理解，展现出更强的通用性和灵活性，成为当前最先进的自然语言处理模型之一。这些模型的共同特征是通过预训练和微调机制，极大地提高了在各种自然语言处理任务中的表现。

与此同时，在文本生成领域，国内对语言模型和预训练模型的研究也取得了显著的进展，展现出丰富的创新成果。例如，王晓峰 \cite{transformer-cn} 基于 Transformer 模型，深入探讨了其在中文文本生成中的应用。他的研究不仅丰富了中文自然语言处理的理论基础，还为实际应用提供了高效的技术支持，推动了该领域的快速发展。这一工作为后续研究者在中文文本生成任务中提供了重要的参考框架，使得相关技术得以在更广泛的场景中应用。

此外，国内还有研究专注于文本生成的基础架构，进一步推动了该领域的创新。李思慧等 \cite{lisihui-textgenerate} 开发了一种新型的基于凸损失函数的离散扩散文本生成模型。该模型充分利用了凸函数锐化最优分布的特性，使得生成过程更加专注于高概率输出，从而显著提高了文本生成的质量。为了进一步提升生成文本的整体效果并降低生成词的重复率，研究团队设计了一种混合感知噪音表。该表通过引入非线性变化的噪音标记，增强了模型在生成过程中的灵活性和适应性。在解码过程中，他们还采用了一种高置信度的确定性去噪策略，有效地减少了生成文本中的噪音干扰。这一策略不仅提升了文本的流畅性和可读性，还增强了生成内容的多样性和创新性。这些研究成果不仅为中文文本生成提供了新的思路和方法，还为未来的研究和应用奠定了坚实的基础，推动了自然语言处理技术在中文语境下的进一步发展。整体而言，国内在文本生成方面的研究正在不断深化，为推动人工智能的进步贡献着重要力量。

国内的研究更多集中于文本生成模型在实际应用领域的探索。胡妍璐 \cite{huyanlu-textgenerate-news} 在分析文本生成技术在新闻生产中的应用基础上，探讨了大数据环境下的新闻文本生成，旨在构建一种新型且更具影响力的新闻生产模式。张蔚琪 \cite{zhangweiqi} 提出了基于孪生架构与多视图匹配的法律问答模型，利用BERT孪生网络架构和注意力池化层生成多个视图的向量表示，通过细粒度的语义交互提升匹配准确性。此外，她将法律问答建模为生成任务，提出了一种基于迁移学习和改进解码算法的生成式法律问答模型，能够在生成过程中识别并替换过于通用的回复，从而提高回答的相关性。进一步地，张蔚琪构建了带有法条知识标注的法律问答数据集，并提出了一种检索知识驱动的生成式模型。该模型结合了知识检索与答案生成，确保生成的回答既具相关性，又能根据最新法律知识进行调整。这些研究成果为法律智能化建设提供了新的思路，推动了法律问答系统的发展。

预训练模型的成功为大语言模型的出现奠定了基础，后者在此基础上进行了多项重要改进。大语言模型通过更大规模的训练数据和参数量，显著提升了模型的表现能力和生成质量。此外，它们采用了更深层次的网络结构和更复杂的训练策略，例如混合精度训练和分布式训练，以提高训练效率和模型的表达能力。同时，大语言模型在上下文理解方面也进行了优化，能够处理更长的文本输入，增强了对上下文的记忆和推理能力。此外，许多大语言模型引入了更先进的自注意力机制和动态计算图，使得模型在生成文本时更加灵活和高效。这些改进使得大语言模型不仅在自然语言生成任务中表现出色，也在理解、翻译和问答等多种应用场景中展现出强大的能力，推动了人工智能技术的进一步发展。在近年来的自然语言处理领域，多个大型语言模型相继被提出，显著推动了文本生成和理解的研究进展。ChatGPT \cite{chatgpt} 是 OpenAI 开发的对话生成模型，基于生成预训练变换器（GPT）架构，经过大规模对话数据的微调，旨在实现自然流畅的人机交互。其设计重点在于上下文理解和生成能力，使其能够在多种主题下进行有效交流。GPT-4 \cite{openai2024gpt4} 是 OpenAI 推出的第四代生成预训练变换器模型，相较于其前身 GPT-3，GPT-4 在模型规模、性能和上下文处理能力上均有显著提升。该模型能够处理更长的文本输入，并在多种复杂任务中表现出更高的灵活性和准确性，尤其在多模态输入（文本和图像）方面展现出新的潜力。Gemini \cite{geminiteam} 是由 Google DeepMind 开发的一种新型大型语言模型，旨在与现有的 AI 模型竞争。Gemini 结合了先进的技术和架构，强调在自然语言理解和生成任务中的表现，展现出在多领域应用中的强大能力。LLaMA \cite{touvron2023llamaopenefficientfoundation, touvron2023llama2openfoundation, grattafiori2024llama3herdmodels} （Large Language Model Meta AI）是 Meta（前 Facebook）推出的一系列开源大型语言模型，旨在提供高效的语言生成和理解能力。LLaMA 关注于可扩展性和性能，支持多种语言和任务，尤其在资源有限的环境中表现出色。这些模型的不断发展为自然语言处理领域的研究和应用提供了新的视角和方法。

% 文心一言, ChatGLM，通义千问，pangu alpha sigma pi, 

近年来，国内在大语言模型的研究领域取得了显著突破。早在OpenAI推出ChatGPT之前，中国的研究人员便已开始对大语言模型进行探索。由于大型企业拥有丰富的显卡资源，因此其研究进展往往领先于学术界。百度的研究团队率先进入这一领域，提出了文心一言模型\cite{sun2019ernieenhancedrepresentationknowledge, sun2019ernie20continualpretraining, sun2021ernie30largescaleknowledge}，该模型具备多模态处理能力，能够有效处理文本、图像等多种数据类型，适应广泛的应用场景。文心一言专门针对中文进行了优化，显著提升了对中文语境的理解和生成效果，展现出卓越的文本生成能力，适合于内容创作和客户服务等多个领域。此外，该模型整合了丰富的知识库，能够在文本生成过程中引用相关知识，从而增强文本的准确性与信息量。其高度的可定制性使得用户能够根据特定需求进行调整，支持实时交互并迅速响应用户输入，从而提升用户体验。这些特性使得文心一言在中文自然语言处理领域具有重要的应用潜力。

在ChatGPT震撼发布后，国内研究人员积极追赶，力求超越OpenAI的研究成果。华为的研究团队提出了盘古模型\cite{zeng2021pangualphalargescaleautoregressivepretrained, wang2023pangupienhancinglanguagemodel, ren2023pangusigmatrillionparameterlanguage}，该模型采用了大规模的预训练和微调策略，在多种语言理解和生成任务中表现出色。此外，盘古模型在中文处理方面进行了针对性优化，能够更准确地理解和生成符合中文语境的内容，具备强大的上下文理解能力，能够在复杂对话和长文本生成中保持连贯性。华为团队还深入研究了模型的可扩展性和效率，使其能够在多种硬件平台上高效运行。总体而言，盘古模型为中文自然语言处理提供了强有力的技术支持，推动了相关应用的发展。

在ChatGPT（GPT-3.5）发布的同时，清华大学的研究团队推出了GLM-130B\cite{zeng2023glm130bopenbilingualpretrained}，在众多流行的英语基准测试中显著超越了ChatGPT的前一版本GPT-3 175B（davinci）。同时，在相关基准测试中，GLM-130B也始终显著优于当时百度提出的文心一言系列模型中的最大中文语言模型ERNIE TITAN 3.0 260B。基于GLM-130B独特的缩放特性，该模型实现了INT4量化，无需后续训练，几乎没有性能损失，成为首个在100B规模模型中实现这一技术的模型。更为重要的是，这一特性使其能够在4×RTX 3090（24G）或8×RTX 2080 Ti（11G）GPU上进行高效推理，成为当时使用100B规模模型所需的经济高效的GPU选择。

在 OpenAI 发布 GPT-4 之后，国内的大语言模型研究如雨后春笋般蓬勃发展。阿里巴巴的研究团队推出了通义千问系列模型\cite{bai2023qwentechnicalreport}。Qwen 是一个全面的开源语言模型系列，涵盖了不同参数规模的多种模型。其中包括基础的预训练语言模型 Qwen 以及经过人类对齐技术微调的聊天模型 Qwen-Chat。基础语言模型在多项下游任务中始终展现出卓越的性能，而聊天模型，尤其是那些通过强化学习（RLHF）技术结合人类反馈进行训练的模型，竞争极为激烈。这些聊天模型具备创建代理应用程序的高级工具使用与规划能力，甚至在处理复杂任务时，如使用代码解释器的大型模型中，也表现出令人瞩目的性能。此外，团队还开发了专用于编码的模型 Code-Qwen 和 Code-Qwen-Chat，以及基于基础语言模型的数学模型 Math-Qwen-Chat。与当时的其他开源模型相比，通义千问系列模型的性能显著提升，略微落后于专有模型。

紧接着，清华大学的研究团队在 GLM-130B 模型的研究中取得了创新进展，开发了 ChatGLM-RLHF 流程 \cite{hou2024chatglmrlhfpracticesaligninglarge}，这是一个基于人类反馈的强化学习（RLHF \cite{kaufmann2024surveyreinforcementlearninghuman}）系统，旨在增强 ChatGLM 与人类偏好的对齐一致性。ChatGLM-RLHF 主要包括三个核心组成部分：人类偏好数据的收集、奖励模型的训练以及策略的优化。为降低大规模训练中的奖励方差，采用了通过融合梯度下降实现模型并行性的策略，并设计了正则化约束以防止 LLM 中的灾难性遗忘。实验结果显示，与监督微调（SFT）版本的 ChatGLM 相比，ChatGLM-RLHF 在对齐任务中取得了显著的改进。例如，在中文对齐任务中，其平均表现比 ChatGLM-SFT 提高了 15\%。这项研究展示了国内在根据人类偏好对齐大语言模型方面的实践，为强化学习实施中的挑战与解决方案提供了宝贵的见解。

随后，研究团队在此基础上提出了 GLM-4 大语言模型家族\cite{glm2024chatglmfamilylargelanguage}，其中包括 GLM-4、GLM-4-Air 和 GLM-4-9B。这些模型代表了国内最具实力的开源模型，基于前三代 ChatGLM 的所有经验和洞察进行训练。GLM-4 模型在十万亿个主要为中文和英文的令牌上进行了预训练，同时还使用了来自 24 种语言的一小部分语料库，主要针对中文和英文的使用进行了对齐。高质量的对齐是通过多阶段的训练后过程实现的，包括监督微调和从人类反馈中学习。评估结果表明，GLM-4 在 MMLU \cite{hendrycks2021measuringmassivemultitasklanguage}、GSM8K \cite{cobbe2021trainingverifierssolvemath}、MATH \cite{hendrycks2021measuringmathematicalproblemsolving}、BBH \cite{suzgun2022challengingbigbenchtaskschainofthought}、GPQA \cite{rein2023gpqagraduatelevelgoogleproofqa} 和 HumEval \cite{peng2024humanevalxlmultilingualcodegeneration} 等多个通用指标上与 GPT-4 进行了密切竞争，甚至在某些方面优于 GPT-4。在 IFEval \cite{zhou2023instructionfollowingevaluationlargelanguage} 的指令遵循测评中，GLM-4 的表现接近于 GPT-4-Turbo，并在长上下文任务中与 GPT-4 Turbo（128K）和 Claude 3 的表现相当。此外，在对齐 Bench 测量的中文对齐方面，GLM-4 亦优于 GPT-4。在实际应用中，GLM-4 在通过 Web 浏览获取在线信息和使用 Python 解释器解决数学问题等任务中，表现与 GPT-4 All Tools 相当甚至更佳，仅在 2023 年便吸引了超过 1000 万次的 HuggingFace 下载。

% qwen2.5, qwen2.5VL qwq32b, deepseek

在 GPT-o1 \cite{wu2024comparativestudyreasoningpatterns} 发布之后，国内再度掀起了追赶的热潮。OpenAI的o1模型展示了推理策略（即测试时的计算方法）对大型语言模型推理能力的显著提升，其在多数数据集上均取得了最佳性能。对此，国内的大语言模型研究者们积极响应，迅速跟进。首先，阿里巴巴公司的研究团队在其早期开发的通义千问系列模型基础上，推出了通义千问2.5（Qwen 2.5）模型  \cite{qwen2025qwen25technicalreport}。这一系列全面的大语言模型（LLM）旨在满足多样化的需求。与之前的版本相比，Qwen 2.5在训练前和训练后阶段均取得了显著进展。在预训练阶段，模型所使用的高质量数据集从7万亿词元扩展至18万亿词元，为常识、专家知识及推理能力的提升奠定了坚实基础。在后训练阶段，研究团队通过100万个样本实施了复杂的监督微调和多阶段强化学习，从而增强了人类偏好，显著改善了长文本生成、结构数据分析及指令遵循能力。为了有效应对多样化的应用场景，研究者们推出了不同规模的Qwen 2.5 LLM系列，开放权重的产品包括基础模型和指令调整模型，并提供量化版本。此外，托管方案中还包括两种混合专家（MoE）变体：Qwen 2.5-Turbo和Qwen 2.5-Plus，均可通过阿里云模型工作室获得。Qwen 2.5在语言理解、推理、数学、编码以及人类偏好对齐等多个基准测试中展现了卓越性能。尤其是开放权重的旗舰Qwen 2.5-72B-Instruct在众多开放和专有模型中表现优异，并与最先进的开放权重模型Llama-3-405B-Instruct \cite{grattafiori2024llama3herdmodels} 竞争，后者的参数量约为其五倍。Qwen 2.5-Turbo和Qwen 2.5-Plus在成本效益方面表现出色，分别与GPT-4o-mini和GPT-4o相抗衡。此外，Qwen 2.5模型为训练专业模型（如Qwen 2.5-Math、Qwen 2.5-Coder、QwQ及多模态模型）提供了重要基础。

在推出这一文本模态的大语言模型之后，阿里巴巴的研究团队为实现与OpenAI的GPT系列模型相媲美的多模态能力，继续深入探索多模态大语言模型的潜力，提出了通义千问2.5 VL \cite{bai2025qwen25vltechnicalreport}。该模型作为Qwen视觉语言系列的最新旗舰，展现了基础功能与创新特性的显著进步。Qwen2.5-VL通过增强的视觉识别、精准的对象定位、强大的文档解析能力及对长视频的理解，实现在理解与世界互动方面的重大突破。Qwen2.5-VL的一个显著特点是其能够通过边界框或点的方式精确定位对象，并具备从图像和表格中提取强大结构化数据的能力。为有效处理复杂输入，该模型引入了动态分辨率处理和绝对时间编码，使其能够通过二级事件定位来处理不同尺寸的图像及持续时间可达数小时的视频。这一设计使得模型能够在无需依赖传统归一化技术的情况下，局部感知空间尺度和时间动态。通过从头训练原生动态分辨率视觉转换器（Vision Transformer, ViT \cite{dosovitskiy2021imageworth16x16words}）并结合窗口注意力机制，Qwen2.5-VL在保持原生分辨率的同时有效降低了计算开销。因此，Qwen2.5-VL不仅在静态图像和文档理解方面表现卓越，作为交互式视觉代理，它能够在实际场景中进行推理、工具使用和任务执行，诸如操作计算机和移动设备等。该模型提供三种不同尺寸，以应对从边缘AI到高性能计算的多样化应用场景。旗舰型号Qwen2.5-VL-72B在文档和图表理解方面与GPT-4o及Claude 3.5 Sonnet \cite{AnthropicModelCA}等最先进模型相当，尤其在这些领域表现出色。此外，Qwen2.5-VL在保持强大语言性能的同时，保留了Qwen2.5 LLM的核心语言能力。

在刚刚过去的一年中，杭州深度求索人工智能基础技术研究有限公司经历了技术的“爆炸性”发展，迅速获得了与OpenAI开发的人工智能相媲美的能力。2024年初，该公司推出了DeepSeek模型 \cite{deepseekai2024deepseekllmscalingopensource}，其研究深入探讨了缩放定律，并提出了独特的发现，这些发现为在两种广泛使用的开源配置（7B和67B）中扩展大规模模型提供了重要支持。在缩放定律的指导下，研究团队引入了DeepSeek LLM，这是一个旨在从长远角度推进开源语言模型的项目。为了更有效地支持预训练阶段，研究人员开发了一个目前包含2万亿词元的数据集，并持续进行扩展。随后，在 DeepSeek LLM Base 模型的基础上，团队实施了监督微调（SFT）和直接偏好优化（DPO），最终创建了DeepSeek Chat模型。评估结果显示，DeepSeek LLM67B在一系列基准测试中超越了LLaMA-270B，尤其在代码、数学及推理领域表现优异。此外，开放式评估结果表明，与GPT-3.5相比，DeepSeek LLM67B在聊天任务中展现了卓越的性能。

在2024年中，杭州深度求索人工智能基础技术研究有限公司再次取得了技术突破，推出了DeepSeek-V2 \cite{deepseekai2024deepseekv2strongeconomicalefficient}，这是一种强大的混合专家（MoE）语言模型，具有经济高效的训练和推理能力。该模型总参数量达到236B，其中每个词元的激活参数为21B，并支持128K词元的上下文长度。DeepSeek-V2采用了多项创新架构，包括多头潜在注意力（MLA）和DeepSeekMoE。MLA通过显著压缩键值（KV）缓存为潜在向量，从而确保高效的推理过程；而DeepSeekMoE则通过稀疏计算以较低的成本训练出强大的模型。与DeepSeek67B相比，DeepSeek-V2在性能上显著提升，同时节省了42.5\%的训练成本，减少了93.3\%的KV缓存，并将最大生成吞吐量提升至5.76倍。该模型在一个由8.1T词元构成的高质量多源语料库上进行了预训练，并进一步实施了监督微调（SFT）和强化学习（RL）以最大限度地发挥其潜力。评估结果显示，即便只有21B的激活参数，DeepSeek-V2及其聊天版本仍在开源模型中展现出顶尖的性能。

在2024年12月，深度求索人工智能基础技术研究有限公司推出了一项重大创新，基于先前开发的DeepSeek模型 \cite{deepseekai2024deepseekllmscalingopensource, deepseekai2024deepseekv2strongeconomicalefficient}，开源了与GPT-4o性能相媲美的DeepSeek-V3模型 \cite{deepseekai2024deepseekv3technicalreport}。该模型是一种强大的混合专家（MoE）语言模型，具有总参数量达到671B，其中每个词元激活了37B参数。为实现高效的推理和训练，DeepSeek-V3采用了多头潜在注意力（MLA）和DeepSeekMoE架构，这些架构在DeepSeek-V2中经过了全面验证。此外，DeepSeek-V3引入了一种负载平衡的无辅助损失策略，并设定了多令牌预测的训练目标，以进一步提升模型性能。研究团队在一个包含14.80万亿多样化且高质量词元的语料库上进行了DeepSeek-V3的预训练，随后进行了监督微调和强化学习阶段，以充分发挥其潜力。综合评估结果表明，DeepSeek-V3的性能超越了其他开源模型，并与领先的闭源模型相当。尽管性能卓越，DeepSeek-V3的完整训练仅需2.788M H800 GPU小时。此外，其训练过程表现出极高的稳定性，研究人员在整个训练期间未遇到任何不可恢复的损失峰值或需进行回滚的情况。

紧接着，在刚刚过去的2025年初，深度求索的研究团队向全球发布了其显著成就，开源了与当今全球最强的OpenAI公司所推出的最佳模型GPT-o1相媲美的DeepSeek-R1模型 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}。此次发布包括了第一代推理模型DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1-Zero是通过大规模强化学习（Reinforcement Learning, RL）训练而成的，且未经过任何初步的有监督微调（Supervised Fine-tuning, SFT），展现出卓越的推理能力。通过强化学习的方式，DeepSeek-R1-Zero自然地表现出许多强大而引人注目的推理行为。然而，该模型也面临可读性差和语言混合等诸多挑战。为了解决这些问题并进一步提升推理性能，研究团队推出了DeepSeek-R1，该模型在强化学习之前结合了多阶段训练和冷启动数据。DeepSeek-R1在推理任务中实现了与OpenAI-o1-1217相当的性能。为了支持学术研究社区，DeepSeek-R1-Zero、DeepSeek-R1以及基于文心一言和LLaMa从DeepSeek-R1蒸馏而成的六个密集模型（1.5B、7B、8B、14B、32B、70B）均被无私地开源。

截至目前，阿里巴巴的研究团队再次发布了其最新的研究成果，名为QwQ-32B \cite{qwq32b}。大规模强化学习（Reinforcement Learning, RL）展现出超越传统预训练和后训练方法的潜力，能够显著提升模型的整体性能。近期的研究表明，强化学习不仅能够增强模型的推理能力，还能促进其在复杂任务中的表现。例如，DeepSeek-R1通过整合冷启动数据与多阶段训练，达成了业界领先的性能，使其具备进行深度思考和复杂推理的能力。在此背景下，研究人员深入探讨了大规模强化学习对大型语言模型智能提升的作用，并开源了阿里巴巴最新的推理模型QwQ-32B。这一模型拥有320亿个参数，其性能与具备6710亿参数（其中370亿参数被激活）的DeepSeek-R1相媲美。这一成果不仅突显了将强化学习有效应用于经过大规模预训练的强大基础模型的潜力，还显示了在推理任务中实现高效性的可能性。此外，QwQ-32B推理模型中集成了与智能体（Agent）相关的能力，使其在使用工具的同时能够进行批判性思考，并根据环境反馈灵活调整推理过程。这种设计理念表明，强大的基础模型与大规模强化学习的结合，或许为实现通用人工智能提供了一条可行的路径。这一研究成果为未来的人工智能发展指明了方向，展示了大规模强化学习在提升模型智能和推理能力方面的巨大潜力。

随着DeepSeek系列模型和Qwen系列模型的相继推出，中国在文本生成领域展现出强大的研究实力，显著推动了全球相关技术的发展。这些模型不仅在架构设计和训练方法上进行了创新，还在推理能力和生成质量上达到了国际领先水平。中国研究团队的这些努力，不仅推动了国内外学术界对文本生成技术的深入探索，也促进了相关应用的多样化发展。通过开源这些先进模型，研究者们为全球学术界提供了宝贵的资源，促进了知识共享和技术交流。因此，可以说，中国在文本生成领域的持续创新和实践，已经为全球研究和应用的进步提供了有力的支持，确立了其在国际舞台上的重要地位。

\subsection{文本分类}
\label{sec:intro-inout-textclassify}

% 按照传统机器学习、深度学习、预训练模型的顺序写，写粗略一点吧

在信息爆炸的背景下，文本分类作为自然语言处理（NLP）领域的一项核心任务，发挥着至关重要的作用。其主要目标是将文本数据自动归类到预设的类别中，广泛应用于情感分析、主题识别、垃圾邮件过滤及新闻分类等领域。在本文中，我们将探讨基于文本分类的方法，以实现对人工智能生成文本来源的检测。随着社交媒体和网络内容的迅速扩展，如何高效且准确地对海量文本进行分类已成为当前研究的一个重要焦点。文本分类的研究方法大致可以划分为三个主要阶段：机器学习、深度学习和预训练模型。

在深度学习方法出现之前，机器学习技术在研究界广泛受到欢迎。彼时，中国的经济条件相对不发达，导致计算机及相关研究领域的发展明显滞后于国外。在国际上，研究人员率先开展了文本分类的相关研究。文本分类的定义是从原始文本数据中提取特征，并基于这些特征进行类别预测。在过去几十年中，针对文本分类的多种模型相继被提出。在传统模型中，朴素贝叶斯（Naive Bayes, NB）\cite{Maron1961AutomaticIA} 是首个被应用于文本分类任务的模型。此后，K-近邻（KNN）\cite{Cover1967NearestNP}、支持向量机（SVM）\cite{Joachims1999TextCW} 和随机森林（RF）\cite{Breiman2001RandomF} 等通用分类模型相继问世，这些模型统称为分类器，并在文本分类领域得到了广泛应用。

进入千禧年后，随着中国经济的迅速发展，越来越多的个人电脑普及，使得更多人能够参与计算机科学领域的研究。这一变革也促进了中国学者在机器学习理论方法研究方面的逐步深入。陈天奇等人\cite{Chen2016XGBoostAS} 提出了极端梯度提升（eXtreme Gradient Boosting, XGBoost）的方法，这是一种创新的稀疏感知算法，旨在处理稀疏数据和加权分位数草图，以实现树学习的近似计算。更为重要的是，他们在论文中提供了关于缓存访问模式、数据压缩及分片的深入见解，以构建可扩展的树提升系统。在此基础上，柯国霖等人\cite{Ke2017LightGBMAH} 提出了轻量级梯度提升机（Light Gradient Boosting Machine, LightGBM），实验结果显示，该方法在多个公共数据集上将传统梯度提升决策树（GBDT）的训练速度提高了超过20倍，同时保持了几乎相同的精度，展现出卓越的性能潜力。除了理论研究，中国学者们还重点关注机器学习的应用研究。面对高校“一站式”学生社区建设中亟需对网络平台事件言论进行分类的问题，孔令怡等人\cite{XDJS202421019} 采用了多种机器学习算法，包括K-近邻、决策树、多项式朴素贝叶斯、逻辑回归、支持向量机、随机森林、轻量级梯度提升及极端梯度提升，构建了针对社区事件言论的分类模型。此外，赵锴等人\cite{ZGKA202410022} 为实现高效且准确的矿床描述文本多标签分类，致力于降低从大量文本中提取细粒度知识的难度，构建了有针对性的标注数据集和相应的机器学习模型。这类应用研究在中国逐渐增多，上述研究以供参考。

随着并行计算速度的显著提升，深度学习方法的研究得到了蓬勃发展。深度神经网络（DNNs）由模拟人脑结构的人工神经元组成，能够自动从数据中提取高级特征。在语音识别、图像处理和文本理解等领域，这些网络相较于传统模型展现出了更为优越的性能。在数据分类过程中，需对输入数据集进行深入分析，包括单标签、多标签、无监督和不平衡数据集等多种类型。根据数据集的特性，将输入的词向量传递至深度神经网络进行训练，直至满足预设的终止条件。模型的性能通过下游任务进行验证，例如情感分类、问答和事件预测等。由于国外在深度学习方法的研究方面起步较早，因而在该领域的研究进展上暂时领先于国内。

在过去几十年中，众多深度学习模型被提出以用于文本分类。文本分类技术的应用领域涵盖情感分析（Sentiment Analysis, SA）、主题标注（Topic Labeling, TL）、新闻分类（News Classification, NC）、问答系统（Question Answering, QA）、对话行为分类（Dialog Act Classification, DAC）、自然语言推理（Natural Language Inference, NLI）以及关系分类（Relation Classification, RC）。多层感知器 \cite{Alsmadi2009PerformanceCO} 和递归神经网络 \cite{10.1145/3234150} 是最早用于文本分类的两种深度学习方法，相较于传统模型，它们在性能上有了显著提升。随后，卷积神经网络（Convolutional Neural Network, CNNs）、循环神经网络（Recurrent Neural Network, RNNs）以及注意力机制也被广泛应用于文本分类任务 \cite{Zhao2018InvestigatingCN, Qin2020DCRNetAD, Deng2021HTCInfoMaxAG}。此外，许多研究者通过改进卷积神经网络、循环神经网络和注意力机制，或者采用模型融合和多任务学习的方法，进一步提升了不同任务下的文本分类性能。

国内在文本分类领域应用深度学习技术的起步相对较晚，但已取得显著成就。在理论研究方面，姚亮等 \cite{Yao2018GraphCN} 探索了图卷积网络在文本分类中的应用。该研究基于单词共现和文档内单词关系构建单个文本图，并为语料库学习一个文本图卷积网络（Text GCN）。其文本图卷积网络通过单词和文档的独热编码（one-hot）进行初始化，随后联合学习单词和文档的嵌入，利用已知的文档类别标签进行监督。李琛等 \cite{Li2021TextGTLGT} 提出了一种创新的半监督框架，称为面向文本的基于图的归纳学习（TextGTL）。该方法在理论指导下优化了图的拓扑结构，并在不同文本图之间实现信息共享。TextGTL还通过图中的密集子结构执行属性空间插值，以高质量特征节点来预测低熵标签，从而实现数据增强。除了在深度学习技术下进行文本分类的研究，国内在应用方面也进行了大量探索。黄瑞等 \cite{JSJA2024S1014} 提出了基于不变图卷积神经网络的文本分类方法，旨在通过GCN实现归纳文本分类。该方法首先为每个文档构建单个图，利用GCN根据局部结构学习细粒度的单词表示，从而有效生成新文档中未见单词的嵌入，并将这些单词节点合并为文档嵌入；接着提取最大限度保留不变类内信息的期望子图，并利用这些子图进行不受分布变化影响的学习；最后通过图分类方法完成文本分类。孙雪松 \cite{1024533250.nh} 在深度学习网络的基础上提出了情感文本分类的方法，采用TextCNN模型捕捉全局特征的同时，利用BiLSTM模型捕捉时序特征，进一步获取文本上下文特征，最终进行特征融合。这一方法增强了深度学习模型的特征提取能力，从而提高了模型在文本分类数据集上的预测准确率。通过多项实验对比，其提出的情感文本分类混合模型在各项指标上相较于现有通用分类模型均取得了显著提升。

% 预训练模型的文本分类

随着深度学习模型规模的不断扩大，预训练方法逐渐受到关注。预训练语言模型能够有效地捕捉全局语义表示，并显著提高自然语言处理任务的性能，包括文本分类在内。这些模型通常采用无监督学习的方法自动提取语义知识，并据此构建预训练目标，以便使机器能够更好地理解和处理语义信息。

国外率先开展了预训练模型的研究。ELMo 模型 \cite{Peters2018DeepCW}、OpenAI 提出的 GPT 模型 \cite{gpt} 以及 BERT 模型 \cite{devlin_bert_2019}，均为此阶段在文本分类领域提出的重要模型。ELMo 模型是一种深度上下文词表示模型，易于与其他模型集成。该模型能够对单词的复杂特征进行建模，并为不同的语言上下文生成相应的表示。其通过双向长短期记忆网络（LSTM）根据上下文单词学习每个单词的嵌入表示。GPT 模型则采用了有监督微调与无监督预训练相结合的策略，以学习通用表示，这些表示在经过有限的适配后可以迁移到多种自然语言处理任务中。此外，目标数据集的领域无需与未标记数据集的领域相似。GPT 的训练过程通常分为两个阶段：首先，通过对未标记数据集进行建模以学习神经网络模型的初始参数；然后，针对特定任务采用相应的有监督目标来调整这些参数。谷歌提出的 BERT 模型通过在每一层中结合左右上下文进行联合调节，对未标记文本进行深度双向表示的预训练，从而显著提升了包括文本分类在内的自然语言处理任务的性能。BERT 利用双向编码器，旨在通过联合调整所有层的上下文来实现深度双向表示的预训练。其在预测被掩码的单词时能够充分利用上下文信息。只需添加一个额外的输出层进行微调，即可构建适用于多种自然语言处理任务的模型，如情感分析、问答系统和机器翻译。相较于这三种模型，ELMo 是一种基于 LSTM 模型 \cite{LSTM} 的特征提取方法，而 BERT 和 OpenAI GPT 则是基于 Transformer 模型的微调方法。此外，ELMo 和 BERT 采用双向训练，而 OpenAI GPT 则是从左到右进行训练。因此，BERT 结合了 ELMo 和 OpenAI GPT 模型的优点，从而取得了更为优越的效果。

在国内，预训练模型的应用主要集中于特定领域的文本分类。陈晨等 \cite{KXJS202429028} 针对油田安全生产的需求及事故隐患特征，提出了一种基于 BERT-BiLSTM 的分类模型，用于油田安全生产隐患文本的主题自动分类。该模型通过 BERT 提取输入文本的字符级特征，生成全局文本信息的向量表示，随后运用双向长短期记忆网络（BiLSTM）对局部关键信息及上下文深层特征进行提取，最终通过 Softmax 激活函数计算得到分类结果。周荣等 \cite{DLXZ20240909001} 针对当前讽刺文本分类中存在的多义词和准确性不足的问题，提出了一种基于 BERT-BiGRU-CNN 的文本分类模型。首先，利用 BERT 预训练模型对短文本进行句子层面的特征向量表示；其次，通过双向门控循环单元（BiGRU）获取文本的全局序列特征；最后，采用卷积神经网络（CNN）提取局部重点特征，以提升模型的特征提取性能。韩博等 \cite{NJYD202403012} 结合标签混淆模型（Label Confusion Model, LCM），提出了一种基于 BERT 和 LCM 的文本分类模型（BLC）。该模型对文本和标签特征进行了进一步处理。充分利用 BERT 每一层生成的句向量和最后一层的词向量，结合双向长短期记忆网络（Bi-LSTM）以生成文本表示，从而替代 BERT 原始的文本特征表示。在进入 LCM 之前，通过自注意力网络和 Bi-LSTM 增强标签之间的相互依赖关系，以提高最终的分类性能。

尽管国内在文本分类研究领域取得了显著进展，并涌现出一系列创新性的理论和应用成果，但与国际领先水平相比，仍存在一定差距。国内学者在算法设计、模型优化及实际应用等方面展现出卓越的能力，推动了相关技术的快速发展。然而，国外在基础理论、数据集构建以及跨领域应用等方面积累了更为丰富的经验与资源。因此，尽管我们在这一领域已取得一定成就，但要全面赶上国际先进水平，仍需持续努力，加强国际间的交流与合作，促进基础研究与应用研究的深入结合，以期在未来的研究中实现更大的突破与创新。

\section{研究内容及主要贡献}
\label{sec:intro-contribution}

在本文中，提出了如下创新点：

\begin{itemize}
    \item 提出了一个专门针对教育领域的文本溯源新任务旨在应对在人工智能生成内容日益普及的时代，利用 TOSWT 数据集追踪教育材料来源。

    \item 创建了教育学中首个可用的句子级别的 AIGT 检测数据集，模型改写文本检测数据集 (TOSWT)，其中包括 5 种最新大模型的生成文本。基于学生写作的短文文本，本数据集构造 prompt 引导大语言模型生成，再经过若干处理，获得了 53,406 条文档级数据和 147,976 条句子级数据。

    \item 在 TOSWT 数据集上微调了几个基准模型以评估其效果，其中 DeBERTa-V3-Large 模型效果最佳。实验结果表明，该模型在文档级数据上的表现较为优异而在句子级数据上表现较差。

    \item 搭建模型改写文本检测系统，基于 DeBERTa-V3-Large 模型，构建了一个高效的文本检测系统。该系统能够快速、准确地识别和追踪教育材料的来源，为教育工作者和研究人员提供了有力的工具。
\end{itemize}

\section{论文组织结构}
\label{sec:intro-paperarchitect}

本文具有以下结构：
\begin{enumerate}
    \item \textbf{第一章}介绍了关于模型改写文本检测这一任务的研究背景及意义、追根溯源其原理文本生成和文本分类的国内外研究现状及主要贡献以及本论文的组织结构。
    \item \textbf{第二章}介绍了回顾了自然语言处理领域的相关研究，重点关注文本生成技术和预训练语言模型的进展。先分析了Transformer架构的优势，再详细讨论了BERT模型的结构及其在预训练和微调过程中的创新。最后，探讨了AI生成文本的检测技术，介绍了多种现有方法及其在实际应用中的效果与局限性。
    \item \textbf{第三章}介绍了详细探讨了模型改写文本检测数据集（TOSWT 数据集）的构建方法，旨在为教育工作者提供一个有效的工具，以分析和追踪学生文本的来源。在数据集的构建过程中，我们以 Learning Agency Lab 发布的 Automated Essay Scoring 2.0 (AES2) 数据集为基础使用正则表达式，我们成功替换了非 ASCII 字符，再使用句子分割技术，细粒度构建数据集。引入了多种大型语言模型，包括 ChatGPT、GPT4o、Gemini、Qwen 和 DeepSeek。最终，构建出的数据集包含 53,328 篇短文和 147,976 个句子，为后续的文本检测研究提供了丰富的数据支持。此外，本章还深入探讨了文本来源追踪的数学化表述，将其视为一个文本分类问题。
    \item \textbf{第四章}介绍了实验结果与分析，包括对比实验等。本章系统地研究了基于预训练语言模型的文本改写检测方法，通过对比实验和消融分析，深入探讨了不同模型架构在文档级和句子级文本检测任务中的表现差异。对比实验结果表明，DeBERTa-V3-Large 在文档级任务中表现最优，准确率达到 85.97\%，显著优于其他对比模型。在任务粒度方面，研究发现文档级和句子级任务呈现出不同的性能特征。所有模型在文档级任务上的表现明显优于句子级任务。在探索性数据分析上，本章提出了基于余弦相似度和评分一致性的双重评估框架。
    \item \textbf{第五章}介绍了模型改写文本检测系统的设计与实现，包括需求分析、系统架构设计、模块和接口设计以及数据库设计。通过对系统的功能进行详细分析，明确了系统的业务需求和功能需求，为后续的系统实现提供了基础。系统采用前后端分离的架构设计，提高了系统的灵活性和可扩展性。通过模块和接口设计，明确了系统各个模块之间的关系和数据传输方式。数据库设计则为系统的数据存储提供了支持。最后，通过用户注册与登录、文本改写功能、模型改写文本检测功能等展示了系统的实际应用效果。
\end{enumerate}