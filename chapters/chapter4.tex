%%==================================================
%% chapter01.tex for BIT Master Thesis
%% modified by pinren lu
%% version: 0.1
%% last update: Dec 25th, 2016
%%==================================================

\chapter{模型改写文本检测方法}
\label{chap:method}

\section{引言}
\label{sec:method-intro}

在第三章中，本文详细介绍了模型改写文本检测数据集（TOSWT 数据集）的构造方法，强调了数据清洗、句子分割以及利用大型语言模型进行文本改写的重要性。这一过程为后续的文本检测任务奠定了坚实的基础。在本章中，我们将深入探讨模型改写文本检测的方法，着重分析基于预训练模型的技术路线，并阐明实验设计及其结果。

随着人工智能技术的飞速发展，尤其是大型语言模型的广泛应用，文本生成的质量和复杂性显著提升。这使得传统的文本检测方法面临新的挑战，尤其是在识别和追踪人工智能生成文本方面。因此，开发有效的模型改写文本检测方法，成为了当前自然语言处理领域的重要研究方向。通过构建有效的检测模型，我们能够更好地识别出文本中可能的人工智能生成部分，从而为教育领域提供支持。

本章的核心目标是介绍两种基于预训练模型的文本检测方法，分别是 RoBERTa 和 DeBERTa 先进的语言模型。这些模型在自然语言理解和生成任务中表现出色，能够通过深层次的语义理解来识别文本的来源。我们将分析这些模型的架构和训练过程，并探讨它们在文本检测任务中的具体应用。

此外，我们将设计一系列实验，以评估所提出的方法在模型改写文本检测中的有效性。这些实验将包括数据集设置、参数配置、评价指标的选择以及对比实验和消融实验的结果分析。通过这些实验，我们不仅能够验证所提出方法的性能，还能为后续研究提供宝贵的经验和数据支持。

通过本章的研究，我们希望能够为模型改写文本检测领域提供新的思路和方法，推动相关技术的发展与应用。同时，我们也期待这些研究成果能够为教育工作者提供切实可行的解决方案，并启发内容审核人员以及其他相关领域的从业者的应用思路，以应对日益复杂的文本来源问题。

\section{基于预训练的模型改写文本检测}
\label{sec:method-pretrain}

随着自然语言处理（NLP）技术的不断进步，基于预训练模型的方法在文本分析和理解任务中取得了显著的成功。预训练模型通过在大量文本数据上进行训练，学习到丰富的语言特征和语义信息，这使得它们在特定任务中表现出色。在模型改写文本检测领域，利用这些预训练模型能够有效地识别和追踪文本的来源，尤其是在面对人工智能生成的文本时。

预训练模型的核心优势在于其强大的上下文理解能力。与传统的特征工程方法相比，预训练模型能够自动提取文本中的深层次特征，从而提高文本分类和检测的准确性。这一过程通常分为两个阶段：首先是在大规模文本数据上进行无监督预训练，然后在特定任务上进行微调。通过这种方式，模型能够适应不同的文本风格和结构，从而增强其在特定应用场景下的性能。

在模型改写文本检测任务中，预训练模型的使用能够帮助我们识别文本中的细微差异，例如句子结构的变化、用词的替换以及语义的调整。这对于判断文本是否经过人工智能模型的改写尤为重要。通过分析文本的上下文信息，预训练模型能够捕捉到潜在的模式和特征，这些模式可能是人类作者与人工智能生成文本之间的显著区别。

在本节中，我们将介绍两种流行且被本课题应用的预训练模型：RoBERTa \cite{liu_roberta_2019} 和 DeBERTa \cite{he_deberta_2021, he2023debertav3improvingdebertausing}。这两种模型在文本理解和生成任务中都表现出色，尤其是在文本分类和检测方面，已被广泛应用于多个研究领域。

%%% TODO: 把下面的 RoBERTa 和 DeBERTa 统统丢进去润色一遍！

\subsection{RoBERTa 模型}
\label{sec:method-pretrain-roberta}

RoBERTa \cite{liu_roberta_2019}（Robustly Optimized BERT Approach）在 BERT \cite{devlin_bert_2019} 模型的基础上进行了多方面的优化，显著提升了模型的整体性能。

在训练策略方面，原始 BERT 模型存在训练不足的问题。为了解决这一缺陷，RoBERTa 通过延长训练步数、采用更大的批次进行训练，并在更广泛的数据集上进行训练。在实验中，较大的批次训练 BERT 模型的结果表明，这种策略能够有效降低训练的困惑度（perplexity），并提高下游任务的准确率。此外，大批次训练更易于通过分布式数据并行实现，从而显著提升了训练效率。

在训练数据的选择上，RoBERTa 使用了总计 160GB 的未压缩文本，涵盖多种语料库，这一数据量远超 BERT 原始训练所用的 16GB。此结果充分表明数据量和多样性在预训练过程中的重要性。此外，BERT 原始实现中，掩码是在数据预处理阶段一次性生成的，形成了静态掩码。尽管通过数据复制增加掩码的多样性，但该方法仍存在局限性。相较之下，RoBERTa 采用了动态掩码机制，即在每次向模型输入序列时生成新的掩码模式。这一方法在长时间训练或处理大规模数据集时表现出明显优势，不仅提高了训练效率，还在实验中显示出相较于静态掩码更优或相当的性能。

在模型输入格式的优化方面，原始 BERT 模型在训练时使用了下一个句子预测（NSP）任务，以提升下游任务的性能。然而，相关研究 \cite{Lample2019CrosslingualLM, Yang2019XLNetGA, Joshi2019SpanBERTIP} 对这一任务提出了质疑。RoBERTa 经过对比实验发现，去除 NSP 损失并采用从单个文档中采样完整句子作为输入，能够匹配或略微提升下游任务的性能。具体而言，限制序列来自单个文档的 DOC-SENTENCES 格式表现优于从多个文档打包的 FULL-SENTENCES 格式，但为了便于与相关研究的比较，最终 RoBERTa 选择了从多个文档打包的 FULL-SENTENCES 格式。此外，RoBERTa 不再使用 BERT 原有的两文档片段拼接并添加 NSP 损失的输入格式，而是采用从一个或多个文档中连续采样完整句子的方式，确保输入总长度不超过 512 个标记。这一格式有助于模型学习长距离依赖关系，从而增强文本理解能力。

在文本编码的改进方面，BERT 原实现使用了一个大小为 30K 的字符级字节对编码 \cite{BPE}（BPE）词汇表，并需要进行启发式分词规则的预处理。RoBERTa 借鉴其他研究，采用了一个包含 50K 子词单元的更大字节级 BPE 词汇表，这一方法无需额外的预处理或分词。尽管在某些任务上性能略有下降，但由于其通用编码方案的优势，RoBERTa 仍被广泛应用于后续的实验中。

通过以上对 BERT 的改进，RoBERTa 在多个自然语言处理任务中展现出卓越的性能，为后续研究提供了强有力的支持。

在本文中，采用 RoBERTa-base 和 RoBERTa-Large 两种大小的 RoBERTa 模型，其参数如表 \ref{tab:roberta-parameter} 所示。表中的参数包括 Transformer 层数、隐藏层维度、注意力头数以及总参数量。这些参数的设计反映了模型的复杂性及其学习能力。较大的模型（RoBERTa-Large）在处理更复杂的语言任务时，能够更好地捕捉深层次的语义关系和上下文信息，而较小的模型（RoBERTa-Base）则在计算效率和资源消耗上具有一定的优势。通过对比这两种模型的性能，我们能够深入探讨不同规模模型在文本理解和生成任务中的表现差异，从而为后续研究提供有价值的参考。

\begin{table*}[htbp]
\centering
\caption{ RoBERTa-Base 和 RoBERTa-Large 参数} \label{tab:roberta-parameter}
\begin{tabular}{lcc}
\toprule
               & \multicolumn{1}{l}{\textbf{RoBERTa-Base}} & \multicolumn{1}{l}{\textbf{RoBERTa-Large}} \\ \midrule
Transformer 层数 & 12                                        & 24                                         \\
隐藏层维度          & 768                                       & 1024                                       \\
注意力头数          & 12                                        & 16                                         \\
总参数量           & 125M                                      & 355M                                      \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{DeBERTa 模型}
\label{sec:method-pretrain-deberta}

目前 DeBERTa 系列模型公开的论文仅有 DeBERTa 模型 \cite{he_deberta_2021} 和 DeBERTa V3 模型 \cite{he2023debertav3improvingdebertausing}，我们的实验中将会使用 DeBERTa V3 模型，本节将会对 DeBERTa 模型对于 BERT \cite{devlin_bert_2019} 和 RoBERTa 模型 \cite{liu_roberta_2019} 的改进进行介绍。

\subsubsection{DeBERTa 模型}

DeBERTa 模型 \cite{he_deberta_2021} 是预训练语言模型演进过程中的一项重要成果，相较于 RoBERTa 和 BERT，其在多个方面进行了显著改进。这些改进主要体现在注意力机制、掩码解码器以及训练方法等方面，从而显著提升了模型的性能和泛化能力。

在注意力机制的设计上，BERT 和 RoBERTa 将每个单词表示为其词嵌入（内容）与位置嵌入的和，这种表示方式在一定程度上融合了内容和位置信息。相较之下，DeBERTa 采用了一种创新的解耦注意力机制，将每个单词分别编码为两个独立的向量，分别代表内容和位置。在计算单词间的注意力权重时，DeBERTa 利用基于内容和相对位置的解耦矩阵进行独立计算。这种方法使得模型能够更精细地捕捉单词之间的关系。以句子 “The dog chased the cat” 为例，当分析 “dog” 与 “cat” 之间的关系时，DeBERTa 不仅考虑它们的语义内容，还能基于相对位置更准确地判断其依赖关系。相较而言，BERT 和 RoBERTa 在处理这种复杂关系时则显得不够细致。

对于序列中位置为 \(i\) 的词元（token），DeBERTa 采用两个向量进行表示，分别为 \(\mathbf{H}_{i}\) 和 \(\mathbf{P}_{i|j}\)。其中，\(\mathbf{H}_{i}\) 表示该词元的内容信息，而 \(\mathbf{P}_{i|j}\) 则表示该词元与位置为 \(j\) 的词元之间的相对位置关系。词元 \(i\) 与 \(j\) 之间的交叉注意力分数的计算可以被分解为四个组成部分，具体如下：
\begin{align}
\label{eq:deberta-attention}
\begin{split}
\mathbf{A}_{i, j} & = \{\mathbf{H}_{i}, \mathbf{P}_{i | j}\} \times \{\mathbf{H}_{j}, \mathbf{P}_{j | i}\}^{\intercal} \\
& = \mathbf{H}_{i}\mathbf{H}_{j}^{\intercal}+\mathbf{H}_{i}\mathbf{P}_{j | i}^{\intercal}+\mathbf{P}_{i | j}\mathbf{H}_{j}^{\intercal}+\mathbf{P}_{i | j}\mathbf{P}_{j | i}^{\intercal}.
\end{split}
\end{align}

换言之，单词对之间的注意力权重可以通过将基于其内容和位置的解耦矩阵应用于四个注意力分数的求和来计算。这四个分数分别对应于内容对内容、内容对位置、位置对内容以及位置对位置的注意力。

在 DeBERTa 之前，相对位置编码的方法在计算注意力权重时，通常依赖于独立的嵌入矩阵来确定相对位置偏差 \cite{Shaw2018SelfAttentionWR, Huang2018MusicTG}。这相当于仅利用公式 \ref{eq:deberta-attention} 中的内容对内容和内容对位置项来进行注意力权重的计算。DeBERTa 模型则认为位置对内容的项同样具有重要性，因为单词对的注意力权重不仅依赖于其内容信息，还受到相对位置的影响。只有同时考虑内容对位置和位置对内容的项，才能对这种关系进行全面建模。鉴于 DeBERTa 使用了相对位置嵌入，因此位置对位置的项在我们的实现中被认为提供的额外信息有限，因此从公式 \ref{eq:deberta-attention} 中去除了该项。

以单头注意力为例，标准的自注意力操作 \cite{transformer} 可以表示为：
\begin{align}
\mathbf{Q} = \mathbf{HW_{q}}, \mathbf{K} &= \mathbf{HW_{k}}, V = \mathbf{HW_{v}}, \mathbf{A} = \frac{\mathbf{QK}^{\intercal}}{\sqrt{d}} \\
\mathbf{H_{o}} &= \text{softmax}(\mathbf{A})\mathbf{V}
\label{eq:transformer-attention1}
\end{align}
其中，$\mathbf{H} \in \mathbb{R}^{N×d}$ 表示输入的隐藏向量，$\mathbf{H_{o}} \in \mathbb{R}^{N×d}$ 是自注意力的输出，$\mathbf{W_{q}}$、$\mathbf{W_{k}}$、$\mathbf{W_{v}} \in \mathbb{R}^{d×d}$ 是投影矩阵，$\mathbf{A} \in \mathbb{R}^{N×N}$ 是注意力矩阵，$N$ 是输入序列的长度，$d$ 是隐藏状态的维度。

记 $k$ 为最大相对距离，$\delta(i, j) \in [0, 2k)$ 为从词元 $i$ 到词元 $j$ 的相对距离，其定义为：
\begin{align}
\delta(i, j)=\begin{cases}
0, & \text{当~} i - j \leq -k \\
2k - 1, & \text{当~} i - j \geq k \\
i - j + k, & \text{其他情况}
\end{cases}
\label{eq:deberta-distance}
\end{align}

在 DeBERTa 中，带有相对位置偏差的解耦自注意力机制可以用公式 \ref{eq:deberta-dis-att} 来表示。在该公式中，\(\mathbf{Q}_{c}\)、\(\mathbf{K}_{c}\) 和 \(\mathbf{V}_{c}\) 分别是通过投影矩阵 \(\mathbf{W}_{q,c}\)、\(\mathbf{W}_{k,c}\) 和 \(\mathbf{W}_{v,c} \in \mathbb{R}^{d \times d}\) 生成的投影内容向量。此外，\(\mathbf{P} \in \mathbb{R}^{2k \times d}\) 表示在所有层中共享的相对位置嵌入向量，该向量在正向传播过程中保持不变。相对位置向量 \(\mathbf{Q}_{r}\) 和 \(\mathbf{K}_{r}\) 则是通过投影矩阵 \(\mathbf{W}_{q,r}\) 和 \(\mathbf{W}_{k,r} \in \mathbb{R}^{d \times d}\) 生成的。
\begin{align} \label{eq:deberta-dis-att}
\begin{split}
    \mathbf{Q}_c = \mathbf{H} \mathbf{W}_{q,c}, 
    \mathbf{K}_c &= \mathbf{H} \mathbf{W}_{k,c}, 
    \mathbf{V}_c = \mathbf{H} \mathbf{W}_{v,c}, 
    \mathbf{Q}_r = \mathbf{P} \mathbf{W}_{q,r}, 
    \mathbf{K}_r = \mathbf{P} \mathbf{W}_{k,r} \\
    \tilde{\mathbf{A}}_{i,j} &= \underbrace{\mathbf{Q}^{c}_{i}{\mathbf{K}^{c}_{j}}^{\intercal}}_{\text{(a) content-to-content}}  
    + \underbrace{\mathbf{Q}^{c}_{i}{{\mathbf{K}^{r}_{\delta(i,j)}}^{\intercal}}}_{\text{(b) content-to-position}}
    + \underbrace{\mathbf{K}^{c}_{j}{{\mathbf{Q}^{r}_{\delta(j,i)}}^{\intercal}}}_{\text{(c) position-to-content}} \\
    \mathbf{H_o} &= \text{softmax}(\frac{\tilde{\mathbf{A}}}{\sqrt{3d}})\mathbf{V}_c
\end{split}
\end{align}
在此，\(\tilde{\mathbf{A}}_{i,j}\) 表示注意力矩阵 \(\tilde{\mathbf{A}}\) 的一个元素，具体对应于词元 \(i\) 到词元 \(j\) 的注意力分数。具体而言，\(\mathbf{Q}_{i}^{c}\) 是矩阵 \(\mathbf{Q}_{c}\) 的第 \(i\) 行，而 \(\mathbf{K}_{j}^{c}\) 则是矩阵 \(\mathbf{K}_{c}\) 的第 \(j\) 行。同时，\(\mathbf{K}_{\delta(i,j)}^{r}\) 表示相对位置向量 \(\mathbf{K}_{r}\) 中与相对距离 \(\delta(i,j)\) 对应的第 \(\delta(i,j)\) 行，\(\mathbf{Q}_{\delta(j,i)}^{r}\) 则是与相对距离 \(\delta(j,i)\) 对应的第 \(\delta(j,i)\) 行。值得注意的是，DeBERTa 在此使用 \(\delta(j,i)\) 而非 \(\delta(i,j)\)，原因在于对于特定位置 \(i\)，位置对内容的计算实际上是基于位置 \(j\) 的键内容相对于位置 \(i\) 的查询位置的注意力权重。因此，此时相对距离应为 \(\delta(j,i)\)。位置对内容项的计算公式为 \(\mathbf{K}_{j}^{c} \cdot {\mathbf{Q}_{\delta(j,i)}^{r}}^{\intercal}\)，而内容对位置的计算方式则类似。

最后，对注意力矩阵 \(\tilde{\mathbf{A}}\) 应用一个缩放因子 \(\frac{1}{\sqrt{3d}}\)。这一缩放因子在模型训练的稳定性方面发挥了关键作用 \cite{transformer}，尤其是在处理大规模预训练语言模型时，其重要性愈加突出。

\begin{algorithm}[htb]
\caption{解耦注意力（Disentangled Attention）}
\label{algo:DA}
%\SetAlgoLined
 \begin{algorithmic}[1]
    \Require {隐藏状态 $\mathbf{H}$, 相对距离嵌入 $\mathbf{P}$, 相对距离矩阵 $\mathbf{\delta}$.}
    内容投影矩阵 $\mathbf{W}_{k,c}$, $\mathbf{W}_{q,c}$, $\mathbf{W}_{v,c}$,
    位置投影矩阵 $\mathbf{W}_{k,r}$, $\mathbf{W}_{q,r}$.
    \Ensure $\mathbf{H}_o$

    \State {$\mathbf{K}_c = \mathbf{H}\mathbf{W}_{k,c}$, $\mathbf{Q}_c = \mathbf{H}\mathbf{W}_{q,c}$, $\mathbf{V}_c = \mathbf{H}\mathbf{W}_{v,c}$,   $\mathbf{K}_r = \mathbf{P}\mathbf{W}_{k,r}$, $\mathbf{Q}_r = \mathbf{P}\mathbf{W}_{q,r}$}
    \State {$\mathbf{A}_{c\rightarrow c} = \mathbf{Q}_c \mathbf{K}_c^{\intercal}$} 
    \For{$i=0,...,N-1$} 
        \State {$\tilde{\mathbf{A}}_{c\rightarrow p}[i,:] = \mathbf{Q}_{c}[i,:] \mathbf{K}_r^{\intercal}$} 
    \EndFor
    \For {$i=0,...,N-1$}
        \For {$j=0,...,N-1$}
        \State {$\mathbf{A}_{c\rightarrow p}[i,j] = \tilde{\mathbf{A}}_{c\rightarrow p}[i,\mathbf{\delta}[i,j]]$} 
        \EndFor
    \EndFor
    \For{$j=0,...,N-1$}
        \State {$\tilde{\mathbf{A}}_{p\rightarrow c}[:,j] = \mathbf{K}_{c}[j,:] \mathbf{Q}_r^{\intercal}$} 
    \EndFor
    \For {$j=0,...,N-1$}
        \For {$i=0,...,N-1$}
        \State {$\mathbf{A}_{p\rightarrow c}[i,j] = \tilde{\mathbf{A}}_{p\rightarrow c}[\mathbf{\delta}[j,i],j]$} 
        \EndFor
    \EndFor
    \State {$\tilde{\mathbf{A}}=\mathbf{A}_{c\rightarrow c} + \mathbf{A}_{c\rightarrow p} + \mathbf{A}_{p\rightarrow c}$}
    \State {$\mathbf{H}_o = \text{softmax}(\frac{\tilde{\mathbf{A}}}{\sqrt{3d}})\mathbf{V}_c$}
 \end{algorithmic}

\end{algorithm}

DeBERTa 采用了一种高效的实现方法。在其之前的研究中，对于长度为 \(N\) 的输入序列，每个词元的相对位置嵌入需要 \(O(N^{2}d)\) 的空间复杂度 \cite{Shaw2018SelfAttentionWR, Huang2018MusicTG, Dai2019TransformerXLAL}。然而，以内容对位置的计算为例，DeBERTa 观察到相对距离 \(\delta(i, j)\) 的范围为 \([0, 2k)\)，并且所有可能的相对位置嵌入实际上构成了矩阵 \(\mathbf{K}_{r} \in \mathbb{R}^{2k \times d}\) 的一个子集。因此，我们能够在所有查询的注意力计算过程中重用 \(\mathbf{K}_{r}\)，从而显著降低了存储需求。

在 DeBERTa 中，预训练阶段将最大相对距离 \(k\) 设置为 512。解耦的注意力权重可以通过算法 \ref{algo:DA} 高效计算。设 \(\delta\) 为根据公式 \ref{eq:deberta-distance} 得到的相对位置矩阵，其中 \(\delta[i, j] = \delta(i, j)\)。在此过程中，我们并不为每个查询分配独立的相对位置嵌入矩阵，而是通过将每个查询向量 \(\mathbf{Q}_{c}[i, :]\) 乘以 \(\mathbf{K}_{r}^{\intercal} \in \mathbb{R}^{d \times 2k}\)（如算法 \ref{algo:DA} 的第 3-5 行），并利用相对位置矩阵 \(\delta\) 作为索引来提取注意力权重（如算法 \ref{algo:DA} 的第 6-10 行）。为了计算位置对内容的注意力分数，我们通过将每个键向量 \(\mathbf{K}_{c}[j, :]\) 乘以 \(\mathbf{Q}_{r}^{T}\) 来获得注意力矩阵 \(\tilde{\mathbf{A}}_{p \to c}[:, j]\) 的列向量（如算法 \ref{eq:deberta-distance} 的第 11-13 行），并最终通过相对位置矩阵 \(\delta\) 作为索引提取相应的注意力分数（如算法 \ref{algo:DA} 的第 14-18 行）。通过这种方法，我们避免了为每个查询分配内存以存储相对位置嵌入，从而将空间复杂度降低至 \(O(kd)\)，仅需存储 \(\mathbf{K}_{r}\) 和 \(\mathbf{Q}_{r}\)。

此外，DeBERTa 还采用了掩码语言建模（Masked Language Modeling, MLM）作为预训练策略。在这一过程中，模型通过利用周围未被掩码的词汇来预测被掩码的词。DeBERTa 在 MLM 任务中充分考虑了上下文词的内容及其位置信息。尽管解耦注意力机制已经整合了上下文词的内容和相对位置，但它并未充分考虑这些词的绝对位置，而在许多情况下，绝对位置对于准确预测至关重要。

以句子“a new store opened beside the new mall”为例，若将“store”和“mall”进行掩码以进行预测，仅依赖局部上下文（如相对位置和周边词汇）可能使模型难以区分句中的“store”和“mall”，因为这两个词相对于“new”的相对位置是相同的。为了解决这一局限性，模型需要将绝对位置作为补充信息纳入考虑。例如，在该句中，主语是“store”而非“mall”，这种句法上的细微差别在很大程度上依赖于单词在句中的绝对位置。因此，在语言建模过程中，考虑单词的绝对位置显得尤为重要。

\begin{figure*}[htbp]
\centering  
\subfloat[BERT 解码层]{
    \includegraphics[trim={0.5mm 0.5mm 0.5mm 0.5mm},clip,width=0.35\linewidth]{figures/Model_BERT_S.jpg}
    % \label{fig:bert-a}
}
\hfill
\subfloat[改进的遮掩解码]{
    \includegraphics[trim={0.5mm 0.5mm 0.5mm 0.5mm},clip,width=0.58\linewidth]{figures/Model_EMD_S.jpg}
    % \label{fig:emd-b}
}

\caption{BERT 与 DeBERTa 解码层的比较}
\label{fig:emd}
\end{figure*}

将绝对位置信息整合到模型中可以通过两种主要方式实现。BERT 模型选择在输入层直接引入绝对位置信息。相较之下，DeBERTa 结构则在所有 Transformer 层之后、但在基于聚合的上下文词内容和位置嵌入解码掩码词的 Softmax 层之前，添加绝对位置信息，如图 \ref{fig:emd} 所示。通过这种设计，DeBERTa 能够在所有 Transformer 层中有效捕捉相对位置信息，而在解码掩码词时则将绝对位置信息作为补充信息进行利用。因此，DeBERTa 将其解码组件称为增强掩码解码器（Enhanced Mask Decoder, EMD）。

\subsubsection{DeBERTa V3 模型}

由于 ELECTRA \cite{clark2020electrapretrainingtextencoders} 中的替换词元检测（Replaced Token Detection, RTD）和 DeBERTa 中的解耦注意力机制已被证实在预训练阶段具有较高的样本效率，DeBERTa V3 因而应运而生。该模型将原始 DeBERTa 中采用的掩码语言模型（Masked Language Modeling, MLM）目标替换为替换词元检测目标，以充分整合后者的优势。

基于 Transformer 的预训练语言模型通常在大规模文本数据集上进行训练，旨在通过自监督学习的目标来获取上下文中单词的表示，这一目标被称为掩码语言建模（Masked Language Modeling, MLM）\cite{devlin_bert_2019}。具体而言，考虑一个序列 \(X = \{x_i\}\)，我们随机选择并掩盖其中 15\% 的词元，从而生成一个损坏后的序列 \(\tilde{X}\)。接下来，我们训练一个由参数 \(\theta\) 表示的语言模型，以基于 \(\tilde{X}\) 来预测被掩盖的词元 \(\tilde{x}\)，以重建原始序列 \(X\)：
\begin{align}
\max_{\theta} \log p_{\theta}(X|\tilde{X}) = \max_{\theta} \sum_{i \in \mathcal{C}} \log p_{\theta}(\tilde{x}_i = x_i|\tilde{X})
\end{align}
在此，\(\mathcal{C}\) 表示序列中被掩盖词元的索引集合。BERT 的研究者建议，对于被掩盖的词元，10\% 应保持不变，另有 10\% 则用随机选择的词元进行替换，其余的词元则用 \([MASK]\) 标记进行替换。

与 BERT 仅依赖单一的 Transformer 编码器并通过掩码语言模型进行训练的方式不同，ELECTRA 采用生成对抗网络（GAN）的框架，利用两个 Transformer 编码器进行联合训练。具体而言，其中一个编码器作为生成器，经过掩码语言模型的训练，而另一个编码器则作为判别器，通过词元级的二元分类器进行训练。生成器的功能在于生成模糊的词元，以替代输入序列中被掩盖的词元。随后，经过修改的输入序列将被送入判别器进行处理。在判别器中，二元分类器的任务是判断相应的词元是原始词元还是由生成器所替换的词元。我们用 \(\theta_G\) 和 \(\theta_D\) 来分别表示生成器和判别器的参数。判别器的训练目标被称为替换词元检测（Replacement Token Detection, RTD）。生成器的损失函数可以表示为：
\begin{align} 
L_{\text{MLM}} = \mathbb{E} \left( - \sum_{i \in \mathcal{C}} \log p_{\theta_G}(\tilde{x}_{i,G} = x_i|\tilde{\mathbf{X}}_G) \right)
\end{align}
在此，\(\tilde{\mathbf{X}}_G\) 表示通过在原始序列 \(\mathbf{X}\) 中随机掩盖 15\% 的词元所生成的输入数据，供生成器使用。

判别器的输入序列是通过基于生成器输出的概率分布，对被掩盖的词元进行采样并替换而构建的。
\begin{align} 
\tilde{x}_{i,D} = \begin{cases} \tilde{x}_{i} \sim p_{\theta_G}(\tilde{x}_{i,G} = x_i|\tilde{\mathbf{X}}_G), & i \in \mathcal{C} \\ x_i, & i \notin \mathcal{C} \end{cases}
\end{align}

判别器的损失函数可表示为：
\begin{align}
L_{\text{RTD}} = \mathbb{E} \left( - \sum_{i} \log p_{\theta_D} \left( \mathbb{1}(\tilde{x}_{i,D} = x_i)|\tilde{X}_D, i \right) \right) 
\end{align}
在此，\(\mathbb{1}(\cdot)\) 表示指示函数，而 \(\tilde{X}_D\) 则是依据公式 3 构建的判别器输入。在 ELECTRA 模型中，损失函数 \(L_{\text{MLM}}\) 和 \(L_{\text{RTD}}\) 进行联合优化，整体损失 \(L\) 定义为 \(L = L_{\text{MLM}} + \lambda L_{\text{RTD}}\)，其中 \(\lambda\) 是判别器损失 \(L_{\text{RTD}}\) 的权重参数，通常在 ELECTRA 中设定为 50。

此外，DeBERTa V3 采用了一种创新的梯度解耦嵌入共享（Gradient-Disentangled Embedding Sharing, GDES）方法，以替代最初在 ELECTRA 中提出的用于替换词元检测的词嵌入共享（Embedding Sharing, ES）策略。这一改进显著增强了 DeBERTa V3 的性能表现。

\begin{figure*}[htbp]
\centering  
\subfloat[嵌入共享]{
\includegraphics[width=0.31\linewidth,trim={0.1cm 11.8cm 18.5cm 0.1cm},clip]{figures/LRTD-a.pdf}
}
\hfill
\subfloat[无嵌入共享]{
\includegraphics[width=0.31\linewidth,trim={0.1cm 11.7cm 18.5cm 0.1cm},clip]{figures/LRTD-b.pdf}
}
\hfill
\subfloat[梯度解耦嵌入共享]{
\includegraphics[width=0.31\linewidth,trim={0.1cm 10.4cm 17.5cm 0.1cm},clip]{figures/LRTD-c.pdf}
}
\caption{不同嵌入共享方法示意图}
\label{fig:es}
\end{figure*}

DeBERTa V3 引入了一种名为梯度解耦嵌入共享（Gradient-Disentangled Embedding Sharing, GDES）的方法，旨在克服传统嵌入共享（Embedding Sharing, ES）和无嵌入共享（No Embedding Sharing, NES）所存在的不足，同时保留其优点。图 \ref{fig:es} 展示了不同嵌入共享策略的示意图。在嵌入共享（ES）策略中，参数 \(\mathbf{E}\)、\(\theta_G\) 和 \(\theta_D\) 在一次反向传播中依据损失函数 \(L_{\text{MLM}} + \lambda L_{\text{RTD}}\) 共同更新。而在无嵌入共享（NES）策略中，参数 \(\mathbf{E}_G\) 和 \(\theta_G\) 首先基于 \(L_{\text{MLM}}\) 进行反向传播更新，然后 \(\mathbf{E}_D\) 和 \(\theta_D\) 则依据 \(\lambda L_{\text{RTD}}\) 进行独立更新。与此不同，梯度解耦嵌入共享（GDES）方法首先根据 \(L_{\text{MLM}}\) 更新 \(\mathbf{E}_G\) 和 \(\theta_G\)，接着则通过 \(\lambda L_{\text{RTD}}\) 和 \(\mathbf{E}_G\) 更新 \(\mathbf{E}_{\Delta}\) 和 \(\theta_D\)。在此过程中，停止梯度操作符 \(sg\) 被应用，以防止判别器对 \(\mathbf{E}_G\) 的更新。

如图 \ref{fig:es}（c）所示，梯度解耦嵌入共享方法允许生成器与判别器之间共享词嵌入，这使得两者能够从同一词汇表中学习并利用嵌入中蕴含的丰富语义信息。然而，与传统的嵌入共享方法相比，梯度解耦嵌入共享方法确保替换词元检测损失不会对生成器的梯度产生影响，从而避免了目标冲突带来的干扰和效率低下的问题。相反，该方法仅利用掩码语言模型损失来更新生成器的嵌入，确保生成器输出的一致性与连贯性。最终，梯度解耦嵌入共享方法能够实现与无嵌入共享相当的收敛速度，同时不牺牲嵌入的质量。

为实现梯度解耦嵌入共享，DeBERTa V3 将判别器的嵌入重新参数化为 \(\mathbf{E}_D = sg(\mathbf{E}_G) + \mathbf{E}_{\Delta}\)，其中停止梯度算子 \(sg\) 防止梯度流经生成器的嵌入 \(\mathbf{E}_G\)，仅更新残差嵌入 \(\mathbf{E}_{\Delta}\)。在训练过程中，\(\mathbf{E}_{\Delta}\) 被初始化为零矩阵，并依据无嵌入共享的训练流程进行模型训练。在每次迭代中，生成器首先为判别器生成输入，并利用掩码语言模型损失更新 \(\mathbf{E}_G\) 和 \(\mathbf{E}_D\)。随后，判别器在生成的输入上进行操作，并通过梯度解耦嵌入共享损失仅更新 \(\mathbf{E}_D\) 的参数 \(\mathbf{E}_{\Delta}\)。训练完成后，\(\mathbf{E}_{\Delta}\) 将被加到 \(\mathbf{E}_G\) 上，得到的矩阵作为判别器的最终嵌入 \(\mathbf{E}_D\)。

DeBERTa V3 进行了广泛的实验，以评估梯度解耦嵌入共享方法相较于嵌入共享和无嵌入共享的有效性。实验结果表明，梯度解耦嵌入共享是一种有效的权重共享策略，适用于结合掩码语言模型和替换词元检测的预训练语言模型。因此，DeBERTa V3 模型在 Kaggle 排行榜和深度学习领域展现出卓越的性能。在本实验中，使用的 DeBERTa 系列模型包括 DeBERTa-V3-Base 和 DeBERTa-V3-Large，其模型参数如表 \ref{tab:deberta-parameter} 所示。

\begin{table*}[htbp]
\centering
\caption{DeBERTa-V3-Base 和 DeBERTa-V3-Large 模型参数} \label{tab:deberta-parameter}
\begin{tabular}{lcc}
\toprule
               & \multicolumn{1}{l}{\textbf{DeBERTa-V3-Base}} & \multicolumn{1}{l}{\textbf{DeBERTa-V3-Large}} \\ \midrule
Transformer 层数 & 12                                           & 24                                            \\
隐藏层维度          & 768                                          & 1024                                          \\
词汇表大小          & 128K                                         & 128K                                          \\
主干网络参数量        & 86M                                          & 304M                                          \\
嵌入层参数量         & 98M                                          & 131M                                \\ \bottomrule         
\end{tabular}
\end{table*}

\section{实验}
\label{sec:method-experiment}

我们设计了如下实验，基于在第 \ref{chap:TOSWT} 章中介绍的模型改写文本检测数据集微调 \ref{sec:method-pretrain-roberta} 节中提及的 RoBERTa-Base 和 RoBERTa-Large 以及在 \ref{sec:method-pretrain-deberta} 节中提及的 DeBERTa-V3-Base 和 DeBERTa-V3-Large 模型。微调后再调用模型在测试集上输出预测标签，最后使用四种指标准确率（Accuracy）、精确率（Precision）、召回率（Recall）和 F1 分数评估预测标签的水准，进而评定微调后的模型能力。

为满足细粒度的数据要求，分别使用文档级别和句子级别的数据分别微调上文提及的四个模型。这些预训练模型参数来源 URL 如表 \ref{tab:method-pretrain_model_url} 所示。

\begin{table*}[htbp]
\centering
\caption{预训练模型参数来源 URL} \label{tab:method-pretrain_model_url}
\begin{tabular}{ll}
\toprule
\textbf{模型}      & \textbf{URL}                                                                 \\ \midrule
RoBERTa-Base     & https://huggingface.co/FacebookAI/roberta-base    \\
RoBERTa-Large    & https://huggingface.co/FacebookAI/roberta-large   \\
DeBERTa-V3-Base  & https://huggingface.co/microsoft/deberta-v3-base  \\
DeBERTa-V3-Large & https://huggingface.co/microsoft/deberta-v3-large \\ \bottomrule
\end{tabular}
\end{table*}

除了该对比实验外，我们还设计了消融实验，在 AES2 数据集评分基础上，进一步探索大语言模型改写文本的能力。有这样两个问题：原始文本对于大语言模型改写文本的影响有多大？大语言模型改写文本后，新文本的文本质量与原始文本质量一致性如何？为深入探索这两个问题，我们同样设计了两个额外的消融实验放置于 \ref{sec:method-experiment-analysis} 中。

\subsection{数据集设置}
\label{sec:method-experiment-dataset}

本实验中数据集采用在第 \ref{chap:TOSWT} 章中介绍的模型改写文本检测数据集。数据集示例如表 \ref{tab:TOSWT-eg} 所示，受单页篇幅所限，这里选取了一个包含两个句子的数据示例，该示例数据评分在其原始 AES2 数据集中被评定为 2 分。

\begin{table*}[htbp]
\BITSetup{ misc / tabularFontSize = 6}
\centering
\caption{模型改写文本检测数据集数据示例} \label{tab:TOSWT-eg}
\begin{tabular}{lp{12cm}}
\toprule
\multicolumn{1}{c}{\textbf{模型}} & \multicolumn{1}{c}{\textbf{文本}}  \\ \midrule
AES2                            & Luke think you should join the   seagoing cowboys program because you can travel all around and see some   pretty nice things like Luke it said in the passage that you take nine trips   to things like Greece,China,you will past the Atlantic Ocean and also you can   make the world a better place and you can be a hero for being nice. Also so if you want to you can help out animals in the passage it said, the   animals had to drink two or three times a day also a way you can help you   could clean the stalls on the ship and they will give you jobs and other   stuff and so you won't think it is boring you can have fun onboard to they   got a lot of things like baseball,and volleyball and the house it is   table-tennis tournaments,fencing,boxing,reading,whittling,and the games make   time go fast and that's why I think you should join the seagoing Cowboys   program. \\ \midrule
GPT3.5                          & Luke thinks you should   join the Seagoing Cowboys program because you can travel all around and see   some pretty nice things. Like Luke said in the passage, you can take nine   trips to places like Greece and China. You will cross the Atlantic Ocean, and   you can make the world a better place by being nice and becoming a   hero. Also, if you want to, you can help out animals. In the passage, it said   that the animals had to drink two or three times a day. Another way you can   help is by cleaning the stalls on the ship. They will give you jobs and other   stuff. And so you won't think it's boring, you can have fun onboard too. They   have a lot of things like baseball, volleyball, table-tennis tournaments,   fencing, boxing, reading, and whittling. The games make time go fast, and   that's why I think you should join the Seagoing Cowboys program.   \\ \midrule
GPT4o                           & Luke believes you should join the seagoing   cowboys program because it allows you to travel extensively and see beautiful   places; as mentioned in the passage, you can undertake trips to destinations   like Greece and China, cross the Atlantic Ocean, and make the world a better   place while becoming a hero by being kind. Additionally, if you wish, you can assist with caring for animals; the   passage noted that the animals need to drink two or three times a day, and   one of the ways you can contribute is by cleaning the stalls on the ship,   where they will provide you with various tasks to keep you engaged, along   with fun activities such as baseball and volleyball, as well as other   pastimes like table tennis tournaments, fencing, boxing, reading, whittling,   which help make the time fly by-these are the reasons I think you should join   the seagoing Cowboys program. \\ \midrule
Gemini                          & Luke believes you should join   the Seagoing Cowboys program because it offers the opportunity to travel   extensively and witness beautiful sights, as he mentioned in the passage that   participants take nine trips to destinations like Greece and China, traverse   the Atlantic Ocean, and contribute to making the world a better place,   potentially becoming a hero through acts of kindness. Additionally, if you're interested, you can assist animals, as the passage   mentioned that they need to drink two or three times daily; you could also   help by cleaning stalls on the ship, and they'll provide you with various   tasks and activities to keep you engaged and prevent boredom, as there are   plenty of recreational options onboard, including baseball, volleyball, table   tennis tournaments, fencing, boxing, reading, whittling, all of which help   pass the time quickly, and that's why I believe you should consider joining   the Seagoing Cowboys program. \\ \midrule
Qwen                            & Luke believes you should join   the seagoing cowboys program because it offers opportunities to travel   extensively and see beautiful places, such as Greece and China, as mentioned   in the passage. You will cross the Atlantic Ocean and contribute to making the   world a better place, thereby becoming a hero by being kind and   helpful. Additionally, if you wish, you can assist animals, as the passage notes   they need to drink two or three times a day. You could help by cleaning the   ship's stalls and taking on various jobs, ensuring the journey is enjoyable.   Onboard activities like baseball, volleyball, table tennis tournaments,   fencing, boxing, reading, and whittling make time pass quickly, which is why   I think you should join the seagoing cowboys program. \\ \midrule
DeepSeek                        & Luke believes you should join   the seagoing cowboys program because it offers the opportunity to travel   extensively and witness beautiful sights, such as Greece and China, while   crossing the Atlantic Ocean, and it allows you to make the world a better place   and become a hero through kindness. Additionally, if you wish, you can assist animals, as mentioned in the   passage, which require drinking two or three times daily, and you can   contribute by cleaning the stalls on the ship, where they provide jobs and   other activities to ensure it's not boring; you can also enjoy onboard   entertainment like baseball, volleyball, table-tennis tournaments, fencing,   boxing, reading, whittling, and games that make time pass quickly, which is   why I recommend joining the seagoing cowboys program. \\ \bottomrule
\end{tabular}
\end{table*}

对于文档级数据，我们将模型改写文本检测数据集分割为训练集、验证集与测试集。这三个被分割后的小数据集在大数据集随机打乱后分别按照文档的个数以 $7:1:2$ 的比例分割出来。待微调的模型在训练集上微调过后，该模型在验证集上得到初步的模型效果，随后选取损失函数最低的模型在测试集上测试，进而得到最终的效果。

而句子级数据中关于数据集的分割步骤也与文档级数据的分割步骤类似。只不过第一步需要先将所有句子分割出来，单独打上标签，按照句子的个数随机打乱顺序后再以 $7:1:2$ 的比例分割为训练集、验证集和测试集。

对于词元个数过长的数据用例，我们将长度大于 512 后面的部分完全截断。不过，这一步是在词元化（tokenizer）的过程中处理的。

\subsection{实验参数设置}
\label{sec:method-experiment-parameter}

所有实验均运行于 Ubuntu Linux 22.04 服务器上。详细实验操作系统环境如表 \ref{tab:os-env} 所示。

\begin{table*}[htbp]
\centering
\caption{实验操作系统环境} \label{tab:os-env}
\begin{tabular}{ll}
\toprule
\textbf{字段}    & \textbf{值}                               \\ \midrule
内核版本           & 6.8.0-49-generic                         \\
构建环境           & buildd@lcy02-amd64-103                   \\
架构             & x86\_64 (64位)                             \\
编译器            & x86\_64-linux-gnu-gcc-12                  \\
编译器版本          & 12.3.0                                   \\
编译器来源          & Ubuntu 12.3.0-1ubuntu1\textasciitilde22.04             \\
链接器   (GNU ld) & GNU Binutils for Ubuntu 2.38             \\
构建编号           & \#49\textasciitilde22.04.1-Ubuntu                       \\
构建类型           & SMP (对称多处理) + PREEMPT\_DYNAMIC（动态抢占模式） \\
构建时间           & Wed Nov 6 17:42:15 UTC 2024              \\
发行版基础          & Ubuntu 22.04.1 LTS                       \\ \bottomrule
\end{tabular}
\end{table*}

本章节全部实验均在拥有 24GB 显存容量的 NVIDIA TITAN RTX 显卡上完成，该显卡详细环境如表 \ref{tab:gpu-env} 所示。

\begin{table*}[htbp]
\centering
\caption{实验显卡环境} \label{tab:gpu-env}
\begin{tabular}{ll}
\toprule
\textbf{字段} & \textbf{值}           \\ \midrule
GPU   型号    & NVIDIA TITAN RTX     \\
GPU   架构    & Turning              \\
显存容量        & 24GB                 \\
TDP   (功耗)  & 320W                 \\
驱动版本        & NVIDIA-SMI 560.35.03 \\
CUDA   版本   & 12.6                 \\ \bottomrule
\end{tabular}
\end{table*}

本章节实验在实现上基于 Anaconda 库中搭建的 Conda 虚拟环境运行 Python 代码，Python 环境中部分重要包版本号如表 \ref{tab:python-env} 所示。

\begin{table*}[htbp]
\BITSetup{ misc / tabularFontSize = 6}
\centering
\caption{实验 Python 环境部分重要包版本} \label{tab:python-env}
\begin{tabular}{cll}
\toprule
\textbf{分类}                  & \textbf{字段}       & \textbf{版本号} \\ \midrule
\multirow{28}{*}{基础}         & conda             & 24.9.2       \\
                             & Python            & 3.12.8       \\
                             & GCC               & 11.2.0       \\
                             & cuda-cudart       & 11.8.89      \\
                             & cuda-cupti        & 11.8.87      \\
                             & cuda-libraries    & 11.8.0       \\
                             & cuda-nvrtc        & 11.8.89      \\
                             & cuda-nvtx         & 11.8.86      \\
                             & cuda-runtime      & 11.8.0       \\
                             & cuda-version      & 12.6         \\
                             & ipython           & 8.30.0       \\
                             & json5             & 0.9.25       \\
                             & matplotlib        & 3.9.2        \\
                             & matplotlib-base   & 3.9.2        \\
                             & matplotlib-inline & 0.1.6        \\
                             & nltk              & 3.9.1        \\
                             & numpy             & 2.0.1        \\
                             & openai            & 1.52.1       \\
                             & pandas            & 2.2.3        \\
                             & pillow            & 11.0.0       \\
                             & pip               & 24.2         \\
                             & regex             & 2024.11.6    \\
                             & sentencepiece     & 0.2.0        \\
                             & scikit-learn      & 1.6.0        \\
                             & scipy             & 1.14.1       \\
                             & tensorboard       & 2.17.0       \\
                             & xlsxwriter        & 3.1.1        \\
                             & yaml              & 0.2.5        \\ \midrule
\multirow{6}{*}{Torch}       & pytorch           & 2.5.1        \\
                             & pytorch-cuda      & 11.8         \\
                             & pytorch-mutex     & 1.0          \\
                             & torchaudio        & 2.5.1        \\
                             & torchtriton       & 3.1.0        \\
                             & torchvision       & 0.20.1       \\ \midrule
\multirow{5}{*}{HuggingFace} & accelerate        & 1.2.1        \\
                             & datasets          & 3.2.0        \\
                             & huggingface-hub   & 0.27.0       \\
                             & tokenizers        & 0.21.0       \\
                             & transformers      & 4.47.1       \\ \bottomrule
\end{tabular}
\end{table*}

在基础的文档级和句子级的数据实验中，我们使用 DeBERTa 和 RoBERTa 作为实验用的模型并微调了四种模型：RoBERTa-Base、RoBERTa-Large、DeBERTa-Base 和 DeBERTa-Large，可变超参数包括批大小（batch size, BS）、学习率（learning rate, LR）、权重衰减系数（weight decay, WD）。在所有实验中，学习率均采用余弦退火衰减算法，并使用 AdamW 算法作为优化器。超参数详见表 \ref{tab:hyper-parameters}，其中，在句子级实验中，大尺寸模型如 RoBERTa-Large 和 DeBERTa-V3-Large 的超参数均经过广泛地搜索长时间实验才找到可以使模型拟合于数据的一组超参数。DeBERTa-V3-Large 由于其总参数量较大，为防止超出显卡显存，因此批大小相较其他实验的 16 个为一批较小，设置为 4 个为一批。

\begin{table*}[htbp]
\centering
\caption{对比试验微调超参数列表}
\label{tab:hyper-parameters}
\begin{tabular}{c|l|ccc}
\toprule
                          & \textbf{模型}   & \textbf{批大小} & \textbf{初始学习率} & \textbf{权重衰减系数} \\ \midrule
\multirow{4}{*}{文档级}    & RoBERTa-Base     & 16                  & $10^{-5}$                & 0.01                  \\
                          & RoBERTa-Large    & 16                  & $10^{-5}$                & 0.01                  \\
                          & DeBERTa-V3-Base  & 16                  & $10^{-5}$                & 0.01                  \\
                          & DeBERTa-V3-Large & 4                   & $10^{-5}$                & 0.01                  \\ \midrule
\multirow{4}{*}{句子级}    & RoBERTa-Base     & 16                  & $10^{-5}$                & 0.01                  \\
                          & RoBERTa-Large    & 16                  & $10^{-6}$               & 0.0001                \\
                          & DeBERTa-V3-Base  & 16                  & $10^{-5}$                & 0.01                  \\
                          & DeBERTa-V3-Large & 4                   & $10^{-6}$               & 0.001                 \\ \bottomrule
\end{tabular}
\end{table*}

\subsubsection{交叉熵损失函数}

交叉熵函数 \cite{CrossEntropy} 是信息论和机器学习领域中的一个基本概念，主要用于量化两个概率分布之间的差异。其数学表达式为：对于真实分布 \( P \) 和预测分布 \( Q \)，交叉熵 \( H(P, Q) \) 定义为：
\(
H(P, Q) = -\sum_x P(x) \log Q(x)
\)
。交叉熵反映了使用分布 \( Q \) 编码从分布 \( P \) 中抽取事件所需的平均比特数。当 \( Q \) 越接近于 \( P \) 时，交叉熵的值将趋向于减少。

在机器学习的应用中，交叉熵常常被用作分类任务的损失函数。例如，在本文讨论的六分类问题中，针对 \( m \) 个样本，该损失函数可以表示为：
\begin{align}
    L=-\frac{1}{m}\sum_{i=1}^m \sum_{j=1}^6 t_{ij}\log(y_{ij})
\end{align}
其中，\( t_{ij} \) 表示真实标签（采用 one-hot 编码），而 \( y_{ij} \) 则是模型的预测概率。这种损失函数设计能够有效地惩罚预测错误，尤其是在与 Softmax 或 Sigmoid 激活函数结合使用时，能够高效地计算梯度，并避免均方误差在学习过程中导致的学习速率下降问题。

交叉熵与信息熵及 KL 散度之间存在密切的关系。信息熵 \( H(P) \) 代表分布 \( P \) 的最佳编码长度，而交叉熵 \( H(P, Q) \) 则是使用分布 \( Q \) 编码 \( P \) 的代价。两者之间的差值即为 KL 散度，反映了两个分布之间的差异。在实际应用中，需要注意数值的稳定性，例如，通过对 \( Q(x) \) 进行裁剪以防止出现 \( \log(0) \) 的错误。

将交叉熵函数作为损失函数的优点在于其理论基础的严谨性与实践中的高效性。例如，在图像分类任务中，交叉熵直接优化预测概率与真实标签之间的匹配程度；而在本文所涉及的自然语言处理领域，它用于评估生成文本的概率分布的质量。

\subsubsection{AdamW 优化器}

AdamW 优化器 \cite{AdamW} 是对原始 Adam 优化器 \cite{Adam} 的一种重要改进，其算法流程如算法 \ref{algo:AdamW} 所示。该算法由 Ilya Loshchilov 和 Frank Hutter 于 2017 年提出，旨在解决传统 Adam 优化器在权重衰减（Weight Decay）处理方面的不足。在原始的 Adam 优化器中，权重衰减是通过将衰减项直接纳入梯度更新中来实现的。这种方法使得权重衰减的效果受到自适应学习率的影响，进而可能削弱正则化的有效性。相比之下，AdamW 优化器通过将权重衰减与梯度更新过程解耦，使其独立于梯度更新步骤，从而能够更准确地实现正则化效果。这一改进不仅提高了优化过程的稳定性，也增强了模型的泛化能力。

\begin{algorithm}[htb]
\caption{AdamW 优化算法} \label{algo:AdamW}
\begin{algorithmic}[1]
    \Require 学习率 $\psi(\text{lr})$, $\beta_1$, $\beta_2$ (betas), 初始参数 $\theta_0$ (params), 目标函数 $f(\theta)$ (objective), $\epsilon$, 权重衰减 $\chi$, amsgrad, maximize
    \Ensure $\theta_t$
    
    \State 初始化: $m_0 \leftarrow 0$ (一阶矩), $v_0 \leftarrow 0$ (二阶矩), $\widehat{v_0}^{max} \leftarrow 0$
    \For {$t = 1$ to $\ldots$}
        \If{maximize}
            \State $g_t \leftarrow - \nabla_\theta f_t(\theta_{t-1})$
        \Else
            \State $g_t \leftarrow \nabla_\theta f_t(\theta_{t-1})$
        \EndIf
        
        \State $\theta_t \leftarrow \theta_{t-1} - \psi \chi \theta_{t-1}$ \Comment{权重衰减}
        
        \State $m_t \leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t$ \Comment{更新一阶矩}
        \State $v_t \leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$ \Comment{更新二阶矩}
        
        \State $\widehat{m_t} \leftarrow m_t / (1 - \beta_1^t)$ \Comment{偏差修正}
        \State $\widehat{v_t} \leftarrow v_t / (1 - \beta_2^t)$
        
        \If{amsgrad}
            \State $\widehat{v_t}^{max} \leftarrow \max(\widehat{v_{t-1}}^{\text{max}}, \widehat{v_t})$
            \State $\theta_t \leftarrow \theta_t - \psi \widehat{m}_t / (\sqrt{\widehat{v_t}^{\text{max}}} + \epsilon)$
        \Else
            \State $\theta_t \leftarrow \theta_t - \psi \widehat{m_t} / (\sqrt{\widehat{v_t}} + \epsilon)$
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}

AdamW 优化器的核心理念在于保留 Adam 优化器所具备的自适应学习率和动量机制，同时对权重衰减的处理方式进行改进。具体而言，AdamW 在参数更新过程中将权重衰减项独立于梯度计算，直接对参数施加衰减。这一方法显著增强了权重衰减的稳定性，有效防止了模型的过拟合现象，并提升了其泛化能力。实验结果表明，AdamW 在多种深度学习任务中均优于传统的 Adam 优化器，尤其在需要强正则化的情境下表现尤为突出。

AdamW 的优势体现在多个方面：首先，它提供了更为有效的正则化机制，从而更好地控制模型的复杂性；其次，在训练深度神经网络时展现出更为稳定的收敛性；最后，它适用于需要长时间训练的任务，如大语言模型（LLM）的预训练。然而，AdamW 也存在一些不足之处，例如对学习率和权重衰减系数的选择较为敏感，这要求进行更多的超参数调节，尤其在处理句子级别较大的模型时，调参工作显得尤为繁琐。

在实际应用中，AdamW 已成为多个深度学习框架（如 PyTorch）的默认优化器之一，特别适合于处理高维参数空间和复杂模型结构。其改进设计不仅保持了 Adam 的高效性，还进一步提升了模型的泛化性能，使其成为现代深度学习训练中的一项重要工具。

\subsubsection{余弦退火调度器}

余弦退火是一种在深度学习领域广泛应用的学习率调度策略，其核心理念是通过余弦函数的动态调整来优化模型的训练过程。该方法最早由 Ilya Loshchilov 和 Frank Hutter 在 2016 年提出的 SGDR 方法 \cite{SGDR} 中引入，随后迅速成为深度学习中的一项重要技术。余弦退火的名称源于其数学实现方式——学习率按照余弦函数曲线从初始值平滑下降至最小值，类似于金属加工中的退火过程，即先进行高温加热，随后缓慢冷却。

从数学原理的角度来看，余弦退火的公式为：
\begin{align}
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{T_{cur}}{T_i}\pi))
\end{align}
其中，\(\eta_{max}\) 和 \(\eta_{min}\) 分别表示学习率的上限和下限，\(T_{cur}\) 是当前的训练步数，而 \(T_i\) 是总的训练步数。这一设计使得学习率在训练初期保持较高的值，以加速收敛，随后逐渐减小，从而便于模型对参数进行精细调整。与传统的阶梯式衰减或线性衰减方法相比，余弦退火提供了更为平滑的过渡，避免了学习率突变所引发的训练不稳定性。

在实际应用中，余弦退火展现出多重优势。首先，其周期性调整特性有助于模型跳出局部最优解，这对于处理多峰损失函数尤为有效。其次，该方法显著减少了人工调参的需求，通过预设的数学规律自动调整学习率，使其适用于各种复杂模型和大规模数据集。实验结果表明，在图像分类、自然语言处理等任务中，采用余弦退火策略的模型通常能实现更快的收敛速度和更优的泛化性能。例如，在 ImageNet 竞赛中，使用该策略的 ResNet 模型表现出了显著的性能提升。

\subsubsection{权重衰减算法}

权重衰减是一种在深度学习领域广泛应用的正则化技术，主要目的是防止模型出现过拟合现象。其基本原理是通过在损失函数中引入一个与权重参数的平方和成正比的惩罚项（即 \( L_2 \) 范数），以约束模型参数的大小。具体的数学表达式为：
\begin{align}
L' = L + \lambda \sum \theta_i^2
\end{align}
在此公式中，\( L \) 表示原始损失函数，\(\theta\) 是模型的权重，而 \(\lambda\) 是正则化系数，用于控制惩罚的强度。这样的设计使得模型在训练过程中更倾向于学习较小的权重值，从而降低模型的复杂度，避免对训练数据中噪声的过度敏感。

从实现机制的角度来看，权重衰减在梯度下降过程中直接影响参数的更新。以随机梯度下降为例，这一操作可以视为在每次迭代中对权重施加线性衰减，因此被称为“权重衰减”。需要指出的是，权重衰减通常不适用于偏置项，因为偏置主要负责输出的平移，对模型复杂性的影响相对较小。

权重衰减的作用机制可以从多个方面进行解释。首先，较小的权重使得模型对输入变化更加鲁棒，从而减少对噪声特征的依赖；其次，通过抑制特征之间的共线性，避免模型对某些特定特征组合的过度依赖；最后，它能够有效防止梯度爆炸问题，从而提升训练的稳定性。实验结果表明，在像 ImageNet 这样的大型数据集上，合理设置 \(\lambda\) 可以使测试误差降低 10\% 至 20\%。

在实际应用中，权重衰减常常与其他技术（如 Dropout 和批量归一化）结合使用。超参数 \(\lambda\) 的选择通常需要通过多次验证来确定，典型取值范围为 \(10^{-4}\) 到 \(10^{-2}\)。在本文中，我们经过多次测试确定了相应的权重衰减超参数。现代深度学习框架（如本文使用的 PyTorch）中内置了权重衰减功能，可以通过前述的 AdamW 优化器中的 weight\_decay 参数轻松启用。值得注意的是，尽管权重衰减与 \( L_2 \) 正则化在数学形式上相似，但其实现机制有所不同：前者直接修改优化过程，而后者则通过损失函数间接施加影响。

\subsection{评价指标}
\label{sec:method-experiment-metric}

本文共使用四种评价指标来评定，分别为\textbf{准确率}（Accuracy）、\textbf{精准率}（Precision）、\textbf{召回率}（Recall）和 \textbf{F1 分数}。这四种评价指标将在下面的小节中进行相应的介绍。由于模型改写文本检测本质上是多分类文本分类任务，因此后三者指标需要与二分类指标有相应变化。由于每个生成模型产生相同数量的文本，并且由于数据量对指标的影响最小，因此采用了对数据量敏感的宏观（macro）方法。

以及会介绍一下在后面的章节 \ref{sec:method-experiment-analysis} 中涉及到的 spearman 系数和 pearson 系数。此二者系数用于衡量生成的文本评估的分数与原始 AES2 数据集中数据分数的相关性与一致性。

\subsubsection{准确率}

准确率（Accuracy）是机器学习中最基本且直观的分类评估指标，主要用于衡量模型整体预测的准确性。其核心定义为模型正确预测的样本数量占总样本数量的比例，数学表达式为：
\begin{align}
    Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{align}
在此公式中，\(TP\)（真正例）和 \(TN\)（真负例）分别表示正类和负类中预测正确的样本数量，而 \(FP\)（假正例）和 \(FN\)（假负例）则代表预测错误的样本数量。例如，在疾病诊断的场景中，如果模型成功识别了100名患者中的90名（包括健康和患病个体），则准确率为90\%，直观地反映了模型的综合判断能力。

\subsubsection{精准率}

精准率（Precision）是评估分类模型性能的重要指标之一，主要用于衡量模型预测为正类的样本中，实际为正类的比例。其数学定义为：
\begin{align}
    Precision = \frac{TP}{TP+FP}
\end{align}
在此公式中，\(TP\)（真正例）表示模型正确预测的正类样本数量，而 \(FP\)（假正例）则代表模型错误地将负类预测为正类的样本数量。该指标尤为关注模型预测为正类的可靠性。例如，在医疗诊断领域，高精准率意味着较少的健康患者被误诊为患病，从而避免不必要的治疗费用。

在本文的实验中，由于模型的文本改写检测本质上是一个多分类文本分类任务，因此精准率指标需要与二分类指标进行相应调整。考虑到每个生成模型产生相同数量的文本，并且数据量对指标的影响较小，因此采用了对数据量敏感的宏观（macro）方法进行评估。

\subsubsection{召回率}

召回率（Recall）是机器学习中用于评估分类模型性能的重要指标之一，主要用于衡量模型对正类样本的识别能力。其数学定义为：
\begin{align}
    Recall = \frac{TP}{TP+FN}
\end{align}
在此公式中，\(TP\)表示真正例，即模型正确预测的正类样本数量，而 \(FN\)则代表假负例，即实际为正类但被错误判定为负类的样本数量。该指标反映了所有真实正类样本中被模型成功识别的比例，因此也被称为“查全率”。在医疗诊断等高风险领域，高召回率意味着漏诊风险的降低。例如，如果癌症筛查模型的召回率达到90\%，则表明90\%的真实患者得到了正确检出。

在本文的实验中，由于模型的文本改写检测本质上属于多分类文本分类任务，因此召回率指标需要与二分类指标进行相应调整。考虑到每个生成模型产生相同数量的文本，并且数据量对指标的影响较小，因此采用了对数据量敏感的宏观（macro）方法进行评估。

\subsubsection{F1 分数}

F1分数是机器学习中评估分类模型性能的关键指标，它通过调和精确率（Precision）与召回率（Recall）来提供全面的性能评估。其计算公式为：
\begin{align}
    F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
\end{align}
在此公式中，精确率用于衡量模型预测为正类的准确性（\(Precision = \frac{TP}{TP + FP}\)），而召回率则评估模型捕捉正类样本的能力（\(Recall = \frac{TP}{TP + FN}\)）。这一设计使得F1分数对精确率与召回率赋予相同的重要性，任一指标的低下都会显著影响整体得分。

在本文的实验中，由于模型的文本改写检测本质上属于多分类文本分类任务，因此F1分数指标需要与二分类指标进行相应调整。考虑到每个生成模型产生相同数量的文本，并且数据量对指标的影响较小，因此采用了对数据量敏感的宏观（macro）方法进行评估。

\subsubsection{Spearman 系数}

Spearman相关系数是一种非参数统计量，用于评估两个变量之间的单调关系强度。该系数由心理学家Charles Spearman于1904年首次提出\cite{Spearman}，其计算基于变量的秩次而非原始数值，这使其特别适合于处理非线性关系或不符合正态分布的数据。与Pearson相关系数不同，Spearman系数不要求变量满足线性假设，并且对异常值具有更高的稳健性。

Spearman相关系数的取值范围为-1到1：当系数接近1时，表明两个变量的秩次完全一致，存在强正相关；接近-1则表示两个变量的秩次完全相反，存在强负相关；而当系数接近0时，则意味着变量之间缺乏单调关系。其经典计算公式为：
\begin{align}
    \rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}
\end{align}
其中，\(d_i\)表示两个变量对应观测值的秩次差，\(n\)为样本量。当数据中存在相同秩次时，需要采用更为复杂的计算方法，此时Spearman系数实际上等同于对秩次变量计算Pearson相关系数。

显著性检验通常通过计算以下两个公式来实现：\(z = \rho \sqrt{n-1}\)（近似服从标准正态分布）或\(t = \rho \sqrt{(n-2)/(1-\rho^2)}\)（服从t分布）。现代统计软件提供了便捷的计算函数，使研究者能够更高效地应用该方法，从而揭示变量之间潜在的关联模式。

\subsubsection{Pearson 系数}

皮尔逊相关系数 \cite{Pearson}（Pearson Correlation Coefficient）是统计学中用于衡量两个连续变量之间线性关系强度与方向的重要指标。该系数由卡尔·皮尔逊（Karl Pearson）于19世纪末提出，其取值范围从-1到1。当系数为1时，表示两个变量之间存在完全正相关，即一个变量的增加伴随另一个变量按比例增加；当系数为-1时，表示完全负相关，即一个变量的增加导致另一个变量按比例减少；而当系数为0时，则表明这两个变量之间不存在线性关系。

从数学角度来看，皮尔逊系数是两个变量的协方差与其各自标准差乘积的比值，其计算公式为：
\begin{align}
r = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2}\sum{(Y_i - \bar{Y})^2}}}
\end{align}
在此公式中，\(\bar{X}\) 和 \(\bar{Y}\) 分别表示变量的样本均值。该公式实质上是对协方差进行标准化处理，从而使得结果不受变量量纲的影响。然而，在实际应用中，该系数对数据的正态分布性有一定要求，并且仅能反映线性关系，因而对于非线性关联可能会导致误判。

根据系数绝对值的大小，相关性的强度可分为五个等级：0.8至1.0为极强相关，0.6至0.8为强相关，0.4至0.6为中等相关，0.2至0.4为弱相关，而0.0至0.2则表示极弱或无相关。值得强调的是，强相关性并不等同于因果关系，可能受到第三方变量或偶然因素的影响。

皮尔逊系数的显著性检验通常通过计算 \(t\) 统计量来实现，其自由度为 \(n-2\)（其中 \(n\) 为样本量）。当 \(p\) 值小于显著性水平（如0.05）时，才能认为相关性具有统计学意义。此外，该系数对异常值较为敏感，且在数据存在相同秩次的情况下需要采用特殊处理方法。与Spearman秩相关系数相比，皮尔逊系数更适合于那些满足线性假设的连续变量，而Spearman系数则能够处理更广泛的单调关系。

\subsection{对比实验结果分析}
\label{sec:method-experiment-main}

本节通过两组对照实验系统评估了RoBERTa和DeBERTa模型在不同粒度文本理解任务中的表现。表 \ref{tab:document} 展示了文档级任务上的微调结果，其中DeBERTa-V3-Large以85.97\%的准确率和85.84\%的F1分数显著领先，验证了大规模预训练模型在处理长文本依赖关系时的优势。相比之下，表 \ref{tab:sentence} 揭示的句子级任务结果呈现出截然不同的特征：所有模型的绝对性能明显下降（最佳准确率仅49.61\%），且DeBERTa-V3-Base在精准率指标上意外超越其大型版本。这种差异凸显了文本处理粒度对模型性能的关键影响——文档级任务受益于丰富的上下文信息，而句子级任务则更依赖精准的局部语义理解。后续分析将深入探讨造成这种差异的内在机制，以及不同模型架构在两类任务中的适应性表现。

\subsubsection{文档级数据微调实验}

表 \ref{tab:document} 详细展示了RoBERTa和DeBERTa两个主流预训练语言模型家族在文档级自然语言处理任务上的微调性能对比结果。通过对准确率（Accuracy）、精准率（Precision）、召回率（Recall）和F1分数四个核心评估指标的全面分析，我们可以得出若干重要发现。

\begin{table*}[htbp]
\caption{RoBERTa 与 DeBERTa 模型在文档级数据上微调结果}
\centering
\begin{tabular}{l|cccc}
\toprule
\textbf{模型}& \textbf{准确率}   & \textbf{精准率}    & \textbf{召回率}    & \textbf{F1}   \\ \midrule
RoBERTa-Base  & 72.88          & 77.26          & 72.89          & 72.73          \\
RoBERTa-Large & 80.92          & 81.84          & 80.92          & 80.55          \\
DeBERTa-V3-Base  & 79.85          & 81.44          & 79.85          & 79.81          \\
DeBERTa-V3-Large & \textbf{85.97} & \textbf{86.31} & \textbf{85.96} & \textbf{85.84} \\ \bottomrule
\end{tabular}
\label{tab:document}
\end{table*}

从整体性能趋势来看，所有评估指标都呈现出明显的层级结构：DeBERTa-V3-Large以显著优势领先，其准确率达到85.97\%，精准率86.31\%，召回率85.96\%，F1分数85.84\%，这四项指标均明显优于其他对比模型。这一结果验证了模型规模和架构改进的双重重要性。值得注意的是，DeBERTa-V3-Base（79.85\%准确率）与RoBERTa-Large（80.92\%准确率）的性能相当接近，这表明DeBERTa系列在基础模型规模下就能达到与更大规模RoBERTa模型相近的性能水平，充分体现了其架构设计的先进性。

深入分析模型架构的影响可以发现，DeBERTa系列采用的分离注意力机制和增强型掩码解码器带来了显著优势。特别是在精准率指标上，DeBERTa-V3-Base（81.44\%）已经超过了RoBERTa-Large（81.84\%），而DeBERTa-V3-Large更是将这一优势扩大到86.31\%。这表明DeBERTa的架构改进特别有利于提升模型预测的精确性，减少假阳性错误的发生。这一特性在需要高精度预测的应用场景（如医疗诊断或金融风险评估）中尤为重要。

从模型规模的角度观察，RoBERTa从Base到Large版本的性能提升幅度为8.04个百分点（准确率从72.88\%到80.92\%），而DeBERTa从Base到Large版本的提升幅度更大，达到6.12个百分点（从79.85\%到85.97\%）。虽然绝对提升值略低，但考虑到DeBERTa-Base的起点更高，这一相对提升仍然非常可观。这证实了增加模型参数规模对性能提升的持续有效性，同时也表明在优质架构设计的基础上，扩大模型规模能带来更显著的效果。

特别值得注意的是各模型在精准率和召回率之间的平衡关系。所有模型的精准率都略高于召回率，这种系统性差异可能反映了当前预训练语言模型在文档级任务中普遍存在的保守预测倾向。RoBERTa-Base的精准率（77.26\%）比召回率（72.89\%）高出4.37个百分点，这一差距在更大规模的模型中有所缩小，DeBERTa-V3-Large的精准率（86.31\%）仅比召回率（85.96\%）高出0.35个百分点。这表明随着模型规模和架构的优化，模型在保持高精确性的同时，也能更好地覆盖正类样本，实现更均衡的性能表现。

F1分数的变化趋势进一步验证了上述发现。作为精准率和召回率的调和平均数，F1分数综合反映了模型的整体分类性能。DeBERTa-V3-Large的F1分数达到85.84\%，比RoBERTa-Large的80.55\%高出5.29个百分点，这一提升幅度超过了准确率的提升（5.05个百分点），说明DeBERTa的优势在综合考虑精确性和全面性时更为明显。

从实际应用的角度来看，这些结果对模型选择具有重要指导意义。在计算资源允许的情况下，DeBERTa-V3-Large显然是首选方案；在资源受限时，DeBERTa-V3-Base能以较小的参数量达到与RoBERTa-Large相当的性能，是更经济高效的选择。此外，精准率普遍高于召回率的现象提示我们，在这些模型的工业部署中可能需要根据具体应用场景的需求，通过调整分类阈值来优化精确性或召回率。

这些实验结果也为未来研究指明了若干方向。首先，DeBERTa架构的成功验证了分离注意力机制在文档级任务中的有效性，值得进一步研究和扩展。其次，模型规模扩大带来的性能提升尚未达到平台期，暗示继续增大模型规模可能仍有潜力。最后，精准率和召回率之间的不平衡现象值得深入分析，可能需要设计专门的损失函数或训练策略来改善。

综上所述，表 \ref{tab:document} 通过系统的对比实验，不仅展示了不同预训练语言模型在文档级任务上的性能差异，更为模型架构设计、规模选择和实际应用提供了重要的实证依据。这些发现将有助于研究人员和工程师在文档级自然语言处理任务中做出更明智的模型选择和应用决策。

\subsubsection{句子级数据微调实验}

表 \ref{tab:sentence} 展示了RoBERTa和DeBERTa系列预训练语言模型在句子级自然语言处理任务上的微调性能对比。通过分析准确率(ACC)、精准率(P.)、召回率(R.)和F1分数四个关键评估指标，我们可以得出以下深入观察和结论。

\begin{table*}[htbp]
\caption{RoBERTa 和 DeBERTa 在句子级数据上微调结果}
\centering
\begin{tabular}{l|cccc}
\toprule
\textbf{模型}& \textbf{准确率}   & \textbf{精准率}    & \textbf{召回率}    & \textbf{F1}   \\ \midrule
RoBERTa-Base      & 45.08          & 50.13          & 45.08          & 43.93          \\
RoBERTa-Large     & 45.69          & 48.38          & 45.69          & 45.44          \\
DeBERTa-V3-Base   & 47.76          & \textbf{51.49} & 47.76          & 46.93          \\
DeBERTa-V3-Large  & \textbf{49.61} & 50.25          & \textbf{49.62} & \textbf{49.25} \\ \bottomrule
\end{tabular}
\label{tab:sentence}
\end{table*}

从整体性能表现来看，所有模型在句子级任务上的表现明显低于文档级任务（对比表 \ref{tab:document}），这反映了句子级理解任务固有的挑战性。DeBERTa-V3-Large依然保持领先地位，其准确率达到49.61\%，F1分数49.25\%，但相比文档级任务85.97\%的准确率有显著下降。这种性能差距凸显了句子级语义理解的复杂性，其中上下文信息有限，模型更难捕捉深层次的语义关系。

值得注意的是，DeBERTa-V3-Base在精准率指标上以51.49\%的表现超过所有其他模型，包括其大型版本DeBERTa-V3-Large（50.25\%）。这一反常现象可能表明，在句子级任务中，基础规模的DeBERTa模型在避免假阳性预测方面具有特殊优势。同时，RoBERTa系列的表现相对较弱，RoBERTa-Large的准确率仅为45.69\%，甚至低于DeBERTa-V3-Base的47.76\%，这说明在句子级任务中，模型架构的改进比单纯增加参数规模更为重要。

深入分析各指标之间的关系可以发现，所有模型的精准率都高于召回率，这与文档级任务中的观察一致，但差距更为显著。RoBERTa-Base的精准率（50.13\%）比召回率（45.08\%）高出5.05个百分点，DeBERTa-V3-Base的差距更是达到3.73个百分点（51.49\% vs 47.76\%）。这种系统性偏差表明，当前预训练语言模型在句子级任务中普遍倾向于保守预测，可能为了避免错误而牺牲了部分正类样本的识别能力。

从模型规模的影响来看，RoBERTa从Base到Large版本的性能提升非常有限，准确率仅增加0.61个百分点（45.08\%到45.69\%），F1分数增加1.51个百分点（43.93\%到45.44\%）。相比之下，DeBERTa从Base到Large版本的提升更为明显，准确率增加1.85个百分点（47.76\%到49.61\%），F1分数增加2.32个百分点（46.93\%到49.25\%）。这一对比强烈表明，在句子级任务中，DeBERTa的架构设计能够更有效地利用增加的模型容量。

特别值得注意的是各模型在F1分数上的表现。作为精准率和召回率的调和平均，F1分数更好地反映了模型的整体分类能力。DeBERTa-V3-Large以49.25\%的F1分数领先，其次是DeBERTa-V3-Base的46.93\%，两者都明显优于RoBERTa对应版本。这一差距说明DeBERTa的分离注意力机制和增强的位置解码设计特别适合处理句子级的语义关系建模。

从实际应用角度考虑，这些结果提供了重要启示。在句子级任务中，DeBERTa-V3-Base可能是性价比最高的选择，它在多个指标上接近甚至超过更大规模的模型，同时计算成本更低。此外，所有模型相对较低的绝对性能分数（均低于50\%）表明，当前的预训练语言模型在句子级理解任务上仍有很大改进空间，可能需要开发专门的架构改进或训练策略。

这些实验结果也提出了若干值得深入研究的问题。首先，为什么DeBERTa-V3-Base在精准率上表现最佳？这是否与其基础规模的参数配置和特定注意力模式的交互有关？其次，为什么模型规模扩大带来的性能提升在句子级任务中不如文档级任务显著？这可能反映了句子级和文档级任务对模型能力的不同需求。最后，如何解释所有模型在句子级任务中表现出的更大的精准率-召回率差距？这可能与句子级数据的特性或评估方式有关。

综上所述，表 \ref{tab:sentence} 通过详实的实验数据，揭示了预训练语言模型在句子级任务中的性能特点和局限。这些发现不仅对模型选择有直接指导意义，也为未来研究指明了改进方向，包括开发更适合句子级理解的架构变体、设计更有效的微调策略，以及深入理解不同粒度文本处理任务的本质差异。这些见解将有助于推动自然语言处理技术在句子级理解任务上的进一步发展。

\subsection{探索性数据分析实验}
\label{sec:method-experiment-analysis}

对于目前提出的数据集，此时产生了两个新问题。如何评估原文对用大语言模型改写后的文本的影响？如何评估大语言模型改写后的文本与原文的质量一致性？为解决这两个问题，因此本文设计了如下两个实验来探索数据分析。

\subsubsection{被改写文本与原文文本相似性}

为解决第一个问题，我们可以使用 DeBERTa-V3-Large 模型分别提取文本特征后，再使用余弦相似度的方式来评估大语言模型改写后的文本与原文的质量一致性。被改写文本与原始文本经过原始 DeBERTa-V3-Large 模型提取特征后的余弦相似度如表 \ref{tab:cos} 所示，展示了不同文本生成模型在语义保持性方面的量化评估结果。需要特别说明的是，本实验中的余弦相似度指标仅用于衡量改写文本与原始文本的语义相似程度，数值越高表明模型对原文的改动越小，但这并不直接等同于模型质量的优劣。AES2作为原始文本的基准参照，其100\%的相似度结果验证了实验设置的有效性，为其他模型的评估提供了理想的上界参考。表中加粗表示该数值为最大值，下划线表示该数值为最小值。

\begin{table*}[htbp]
\centering
\caption{被改写文本与原始文本经过原始 DeBERTa-V3-Large 模型提取特征后的余弦相似度。}
\begin{tabular}{l|c|c}
\toprule
         & \multicolumn{2}{c}{\textbf{余弦相似度}} \\
\textbf{模型} & \textbf{训练改写集}      & \textbf{测试改写集}    \\ \midrule
AES2     & 100.00                 & 100.00                \\
GPT3.5   & \textbf{96.71}         & \textbf{96.77}        \\
GPT4o    & 96.39                  & 96.39                 \\
Gemini   & 96.08                  & 95.99                 \\
Qwen     & \uline{95.55}          & \uline{95.53}         \\
Deepseek & 95.71                  & 95.68                 \\ \bottomrule
\end{tabular}
\label{tab:cos}
\end{table*}

从实验结果来看，所有测试模型在两个改写数据集上的表现呈现出高度一致性。GPT3.5在训练集和测试集上分别达到96.71\%和96.77\%的相似度，这一微小差异（仅0.06个百分点）证实了实验结果的可靠性。值得注意的是，虽然标注为"训练改写集"和"测试改写集"，但实际上这些数据并未用于任何模型的训练过程。GPT4o 以 96.39\% 的稳定表现紧随其后，而 Gemini、Deepseek 和 Qwen 等模型的表现则相对接近，集中在95.5\%-96.1\%的区间内。

这些数据揭示了当前大语言模型在文本改写任务中的一些共性特征。首先，各模型普遍保持了较高的语义相似度（均超过95\%），这说明主流大语言模型都具备较强的语义理解与保持能力。其次，约4-5个百分点的性能差距反映了不同模型在改写策略上的差异：相似度较高的模型（如GPT3.5）可能更倾向于保守的改写策略，而相似度稍低的模型（如Qwen）可能在改写过程中进行了更大胆的语义重组或表达创新。需要强调的是，这种差异并不必然代表模型质量的优劣，而是体现了不同的设计取向——较高的相似度适合需要严格保持原意的场景，而较低的相似度可能带来更具创造性的改写结果。

从技术角度来看，这些结果引发了一些值得深入探讨的问题。为什么不同模型之间会存在这种系统性差异？可能的影响因素包括：模型预训练阶段使用的语料特性、微调策略的差异、解码过程中的温度参数设置等。特别是GPT3.5与其后续版本GPT4o之间的表现差异，可能反映了OpenAI在不同版本迭代过程中对"忠实度"与"创造性"这两个维度的权衡调整。此外，所有模型与理想值100\%之间存在的差距也表明，即使是当前最先进的大语言模型，在文本改写过程中也难以完全避免语义信息的细微变化。

在实际应用层面，这些发现为模型选择提供了重要参考。在需要严格保持原意的场景（如法律文书、学术论文的改写）中，相似度更高的GPT3.5可能是更稳妥的选择；而在允许一定语义灵活度的场景（如创意写作、广告文案生成）中，相似度稍低但可能更具创造性的其他模型也值得考虑。同时，这些结果也提示我们，在评估文本改写模型时，应该根据具体应用场景的需求，在"忠实度"与"创造性"之间寻找合适的平衡点，而非简单地追求单一指标的最大化。

\subsubsection{被改写文本与原文质量一致性}

如何评估大语言模型改写后的文本与原文的质量一致性呢？解决第二个问题的方法为，我们可以将原始的 AES2 数据集中的评分作为衡量文本质量的依据，将 DeBERTa-V3-large 模型在原始 AES2 数据集上微调后，再生成所有文本的评分。最后依靠 Spearman 系数和 Pearson 系数来评价大语言模型改写后的文本与原文的质量一致性。表 \ref{tab:score-set} 详细列出了本实验所采用的关键参数配置和模型设置。在模型架构方面，我们选择了DeBERTa-V3-Large作为基础模型，该模型通过引入分离注意力机制和增强的位置编码，在多项自然语言处理任务中表现出色。实验数据集采用AES2，这是一个经过专业标注的语义评估基准数据集，特别适合用于文本语义相似性分析任务。

\begin{table*}[htbp]
\centering
\caption{评分一致性实验设置} \label{tab:score-set}
\begin{tabular}{lc}
\toprule
\textbf{字段}   & \textbf{值}   \\ \midrule
模型            & DeBERTa-V3-Large \\
数据集          & AES2             \\
初始学习率       & $10^{-5}$        \\
权重衰减权重     & 0.01             \\
批大小          & 4                \\
调度器          & 余弦退火           \\
损失函数        & 交叉熵函数         \\
优化器          & AdamW            \\ \bottomrule
\end{tabular}
\end{table*}

在训练参数设置方面，我们采用了较为保守的初始学习率 $10^{-5}$，这一选择基于预实验的结果，既能保证模型参数的有效更新，又能避免训练过程中的不稳定性。权重衰减系数设为0.01，这一正则化设置有助于防止模型过拟合。考虑到模型规模和显存限制，我们将批大小设置为4，这一较小的批处理规模是由于 DeBERTa-V3-Large 需要占用较大显存导致不得不设置得很小。这样虽然会降低训练速度，但也能提高梯度更新的稳定性。

训练过程中采用了余弦退火学习率调度策略，该策略能够实现学习率的平滑衰减，在训练初期保持较大的学习率以加快收敛，在后期自动降低学习率以获得更精细的参数调整。损失函数选用标准的交叉熵函数，这是分类任务中最常用的目标函数。优化器采用 AdamW，这是 Adam 优化器的改进版本，通过正确处理权重衰减，在保持自适应学习率优势的同时，能获得更稳定的训练效果。

这些超参数的选择经过了多次实验验证，在保证模型性能的同时，也考虑了训练效率和稳定性。DeBERTa-V3-Large 模型与 AES2 数据集的组合，以及上述训练参数的配置，共同构成了一个平衡而高效的实验设置，为后续的语义相似性分析提供了可靠的技术基础。这些参数设置既参考了相关领域的主流做法，也针对具体任务需求进行了适当调整，体现了实验设计的科学性和严谨性。

表\ref{tab:spearman}展示了基于 AES2 数据集微调后的 DeBERTa-V3-Large 模型在不同改写文本检测数据集上的评估结果，分别通过 Spearman 相关系数和 Pearson 相关系数来衡量模型评分与 AES2 数据集人工评分之间的一致性。实验结果表明，不同改写模型生成的文本在语义保持性方面存在显著差异，这些差异在训练集和测试集上呈现出不同的分布模式。由于微调模型是在 AES2 模型的训练集上进行训练，因此基于训练集文本被大语言模型改写后的文本可能在某种程度上被微调模型见过，故而我们将数据集分为了微调模型可能见过的训练改写集和一定没见过的测试改写集。

\begin{table*}[htbp]
\centering
\caption{使用 AES2 数据集微调后 DeBERTa-V3-Large 模型在模型改写文本检测数据集上文本评分与 AES2 数据集评分的 spearman 和 pearson 系数}
\begin{tabular}{l|cc|cc}
\toprule
\textbf{} & \multicolumn{2}{c}{\textbf{训练改写集}}    & \multicolumn{2}{c}{\textbf{测试改写集}} \\
\textbf{模型} & \textbf{Spearman}  & \textbf{Pearson} & \textbf{Spearman} & \textbf{Pearson}  \\ \midrule
AES2      & 92.70              & 92.38            & 78.26             & 78.10             \\
GPT3.5    & 84.42              & 84.40            & \uline{76.97}     & 76.77             \\
GPT4o     & \textbf{84.83}     & \textbf{84.54}   & 77.47             & 77.17             \\
Gemini    & 83.72              & 83.46            & 77.18             & \uline{76.44}     \\
Qwen      & \uline{83.01}      & \uline{82.68}    & \textbf{78.20}    & \textbf{77.20}    \\
Deepseek  & 83.63              & 83.35            & 77.12             & 76.65             \\ \bottomrule
\end{tabular}
\label{tab:spearman}
\end{table*}

从整体表现来看，模型在 AES2 数据集上的原始文本在训练改写集与测试改写集上取得了最优异的成绩，Spearman和Pearson系数分别达到92.70\%和92.38\%，这一数据实际上是在说明微调后 DeBERTa-V3-Large 模型的微调程度较为优异。然而，该模型在测试改写集上的性能出现明显下降（Spearman 78.26\%，Pearson 78.10\%），这表明评估模型的泛化能力仍存在提升空间。在商业大模型比较中，GPT4o 模型生成的文本在训练改写集上表现最为突出（Spearman 84.83\%，Pearson 84.54\%），而在开源模型中，Qwen 则在测试改写集上展现出相对优势（Spearman 78.20\%，Pearson 77.20\%），这种差异可能反映了不同模型在文本改写策略上的本质区别。

深入分析各模型的跨数据集表现可以发现几个重要现象。首先，所有模型在测试集上的相关系数均低于训练集，这体现了基于 AES2 训练集生成的文本确实在某种程度上被微调模型 DeBERTa-V3-Large 见过。其次，GPT 系列模型（GPT3.5和GPT4o）在两个数据改写集上的质量都保持了相对稳定的表现，这可能得益于其强大的预训练基础和稳健的文本生成能力。相比之下，Gemini和Deepseek等模型虽然训练集表现尚可，但在测试集上的下降幅度更为明显，特别是在 Pearson 系数方面（Gemini 测试集 Pearson 76.44\%）。

从统计角度来看，Spearman 和 Pearson 系数之间普遍存在 0.2-0.3 个百分点的差异，这种微小差距表明模型评分与人工评分之间既保持了较好的线性关系（Pearson），也维持了稳定的单调相关性（Spearman）。特别值得注意的是，Qwen 模型在测试集上的 Spearman 系数（78.20\%）甚至十分接近于 AES2 数据集中的原始文本（78.26\%）。

然而，表中也存在着一些问题。例如，Qwen 模型在测试改写集上拥有与 AES2 最高的评分一致性（Spearman 78.20 和 Pearson 77.20），但是其在训练改写集上却拥有最低的评分一致性（Spearman 83.01 和 Pearson 82.68）。这一对比说明本实验仍存在某些潜在问题，可能是评估过程中存在未被发现的偏差因素，需要进一步分析模型在不同数据子集上的具体表现差异才能得出确切结论。

\section{本章小结}
\label{sec:method-conclusion}

本章系统地研究了基于预训练语言模型的文本改写检测方法，通过对比实验和消融分析，深入探讨了不同模型架构在文档级和句子级文本检测任务中的表现差异。主要研究内容和结论总结如下：

在模型架构方面，本章详细分析了 RoBERTa 和 DeBERTa 系列模型的技术特点。实验结果表明，DeBERTa-V3-Large 在文档级任务中表现最优，准确率达到 85.97\%，显著优于其他对比模型。这验证了其解耦注意力机制和增强掩码解码器在处理长文本依赖关系时的优势。值得注意的是，DeBERTa-V3-Base 以较小的模型规模达到了接近 RoBERTa-Large 的性能水平，体现了架构创新的重要性。

在任务粒度方面，研究发现文档级和句子级任务呈现出不同的性能特征。所有模型在文档级任务上的表现明显优于句子级任务，这反映了丰富的上下文信息对模型性能的积极影响。特别地，在句子级任务中，模型规模扩大带来的性能提升相对有限，而架构差异的影响更为显著，这为不同应用场景下的模型选择提供了重要参考。

在探索性数据分析上，本章提出了基于余弦相似度和评分一致性的双重评估框架。实验发现，在文本相似性上，Qwen 具有最大的修改文本的倾向，而 OpenAI 的 GPT3.5 模型和 GPT4o 模型则在原始文本上仅作小修改。而在以评分作为文本质量代表的一致性实验中，各个大语言模型生成文本的评分均具有与原始文本评分较高的一致性，表明原始文本的文本质量会极大地影响大语言模型改写文本的文本质量。

然而，研究也揭示了当前方法的一些局限性。首先，模型在测试集上的性能下降表明泛化能力仍需提升；其次，句子级任务的整体性能偏低（最佳准确率仅49.61\%）说明现有方法在细粒度语义理解方面仍有改进空间；最后，在探索性数据分析实验中，被改写文本与原文质量一致性实验仍需改进。

这些发现为后续研究指明了多个方向：在文本质量一致性实验中开发更鲁棒的评估模型以提高跨数据集一致性；设计专门针对句子级任务的改进架构；探索更精细的评估指标来捕捉文本改写的多维特性。同时，实验结果也为实际应用中的模型选择提供了实证依据，特别是在需要权衡计算成本和性能表现的场景中。

总的来说，本章通过系统的实验设计和深入的分析，为模型改写文本检测领域提供了重要的方法学参考和技术基础，以推动相关技术在教育质量评估等实际应用中的发展。